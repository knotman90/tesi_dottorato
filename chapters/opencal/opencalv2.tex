\chapter[OpenCAL - The Open Computing Abstraction Layer]{The Open Computing Abstraction Layer for Extended Cellular Automata and the Finite Differences Method}
\label{ch:opencal}
\dictum[Gottfried Leibniz]{There are two kinds of truths: those of reasoning and those of fact. The truths of reasoning are necessary and their opposite is impossible; the truths of fact are contingent and their opposites are possible.}
\vskip 1em

\lettrine[lines=3,lhang=0.33,lraise=0,loversize=0.15]{T}{his} chapter introduces \texttt{OpenCAL}, an open source computing
abstraction layer defining a domain specific language for Extended
Cellular Automata and the Finite Differences method (see chapters \ref{ch:CA} and \ref{ch:FDM} at page \pageref{ch:CA} and \pageref{ch:FDM} repectively). 
Different implementations have been developed, which allow for transparent
parallelism and are able to exploit multicore CPUs and manycore
devices like GPUs, thanks to the adoption of \texttt{OpenMP} and \texttt{OpenCL},
respectively, as well as distributed memory architectures and/or multiple GPUs concurrently.
System software architecture is presented and the
underlying adopted data structures and algorithms are described in
detail. Numerical correctness and efficiency have been assessed by
considering the well known $SciddicaT$ Computational Fluid Dynamics
landslide simulation model as reference example.  
Moreover, a comprehensive study has been performed to device the best platform
for execution as a function of numerical complexity and
computational domain extent. Obtained results have highlighted the
\texttt{OpenCAL} suitability for numerical models development and their execution on  the most suitable high-performance parallel computational device.

\section{Introduction}
\label{sec:opencal_introduction}
Scientific Computing \cite{golub2014scientific} is a broad and constantly growing multidisciplinary research field that uses formal paradigms to study complex problems and solve them through simulation by using advanced computing techniques and capabilities.

Different formal paradigms have been proposed to provide the
abstraction context in which problems are formalized. Partial
Differential Equations (PDEs) were probably the first to be largely
employed for describing a wide variety of phenomena. Unfortunately,
PDEs can be analytically solved only for a small set of simplified
problems \cite{Mazumder20161} and Numerical Methods have to be
employed to obtain approximate solutions for real situations. Among
them, the Finite Differences Method (FDM) was one of the first
considered, still currently employed, to address a wide variety of
phenomena such as acoustics \cite{Chaigne19941112, Branski2014},
heat \cite{Rana2012212, Sahin200619}, computational fluid dynamics
(CFD) \cite{Chang1990317, Deng201390}, and quantum mechanics
\cite{Hu2015640, Farrokhabadi201467}.
 
Besides other solutions proposed for numerically approximating PDEs like, for instance, Finite Elements \cite{Hutton:2003} and Finite Volume Methods \cite{Moukalled:2015}, further formal paradigms were more recently proposed for modeling complex systems. Among them, Cellular Automata (CA) \cite{vonNeumann:1966:TSA:1102024} are Turing-equivalent \cite{Codd:1968:CA:1098682, Cook2004} parallel computational models. CA are widely studied from a theoretical point of view \cite{Wolfram-1984, Langton-1990b, wolfram2002, Ninagawa201542}, and their application domains vary from  Artificial Life \cite{Langton-1986, Beer2004309} to Computational  Fluid Dynamics \cite{Frish&al-1986, McNamara&Zanetti-1988,Higuera&Jimenez-1989, Aidun2010439}, besides many others. In the 80s, an extension of the original CA formalism was proposed to better model and simulate a specific set of complex phenomena \cite{DiGregorio&Serra-1999}. Such an extension is known as Complex
or Multi-Component Cellular Automata and was applied to the simulation of debris flows \cite{D'Ambrosio2003545,avolio2013sciddica}, lava flows \cite{D'Ambrosio2012533,D'Ambrosio2012317, Oliverio2011271, D'Ambrosio2006452},
pyroclastic flows \cite{Avolio2006897, Crisci20051019}, forest fires spreading \cite{Arca2015, Avolio2014209}, hydrologic and eco-hydrologic modeling \cite{Mendicino2015128,	Ravazzani2011634, Cervarolo2010205}, soil erosion
\cite{D'Ambrosio200133}, crowd dynamics \cite{Luba2016, Was&al-2015,Was2014199}, urban dynamics \cite{Blecic2015}, besides others. 
Please refers to Chapter \ref{ch:CA} for an extensive introduction on Cellular Automata and XCA.

Independently from the adopted formal paradigm, the simulation of
complex systems often requires Parallel Computing. OpenMP is the
most widely adopted solution for parallel programming on shared
memory computers \cite{Chapman:2007:UOP:1370966}. It fully supports
parallel execution on multi-core CPUs and, starting from the 4.0
specification, also includes support for accelerators like graphic
processing units (GPUs) or Xeon Phi co-processors. Unfortunately,
compilers like gcc currently do not fully support the OpenMP most
recent specifications and, in practice, OpenMP-based applications
still mainly run on CPUs \cite{Oliverio2011271, Amritkar2014501,
pop:hal-00786675}. However, in recent years, general purpose
computing on graphic processing units (GPGPU), which exploits GPUs
and many-core co-processors for general purpose computation, has
gained wide acceptance as an alternative solution for
high-performance computing, resulting in a rapid spread of
applications in many scientific and engineering fields
\cite{Owens200780}. Most implementations are currently based on
Nvidia CUDA (see e.g. \cite{Blecic2013, D'Ambrosio2013630,
	DiGregorio20131183, D'Ambrosio201230}), one of the first platforms
proposed to exploit GPUs computational power on NVIDIA hardware. An
open alternative to CUDA is OpenCL \cite{Stone201066}, an
Application Program Interface (API) originally proposed by Apple and
currently managed by Khronos Group for parallel programming on
heterogeneous devices like CPUs, GPUs, Digital Signal Processors
(DSPs), and Field-Programmable Gate Arrays (FPGAs). Interest in
OpenCL is continuously growing and many applications can already be
found in literature \cite{Macri2015328, Bedorf20122825, Du2012391,
	Brown2011898}. However, an OpenCL parallelization of a scientific
application is often a non-trivial task and, in many cases, requires
a thorough refactorization of the source code. For this reason, many
computational layers were proposed, which make many-core
co-processors computational power easier to be exploited. For
instance, ArrayFire \cite{Malcolm2012} is a mathematical library for
matrix-based computation such as linear algebra, reductions, and
Fast Fourier transform; clSpMV \cite{Su2012353} is a sparse matrix
vector multiplication library; clBlas \cite{clBlas-2016} is an
OpenCL parallelization of the Blas linear algebra library. Examples
of higher level computational layers, which provide the abstraction
of formal computational paradigms, are: OPS~\cite{Reguly201458,
	Jammy2016450} and OP2 \cite{Giles20131451, Reguly20161265}, which
are open-source frameworks for the execution of structured and
unstructured grid applications, respectively, on clusters of GPUs or
multi-core CPUs; AQUAgpusph \cite{Cercos-Pita2015295}, which is a
smoothed-particle hydrodynamics solver; ASL \cite{asl}, an
accelerated multi-physics simulation software based, among others,
on the Lattice Boltzmann Method; CAMELot
\cite{dattilo2003simulation, d2007parallel} and libAuToti
\cite{spingola2008modeling}, which are a proprietary simulation
environment and an efficient parallel library for XCA model
development, respectively.

Among the above cited softwares, OPS, ASL and CAMELot probably are
the most similar to OpenCAL in terms of modeling and development
approach, and could be considered as possible alternatives to the
library proposed in this thesis.
In particular, OPS provides a straightforward Domain Specific Language for structured grid-based modeling, even if it does not refer to any specific abstract computational formalism.  
Its main characteristic consists in
allowing to obtain different parallel versions of a computational
model starting from its serial implementation, thanks to a seamless
code-generator approach.  MPI-based distributed memory, as well as
CUDA and OpenCL many-core versions can be obtained in this way, with
a minimal effort by the developer.  Conversely, ASL provides
different higher level modeling abstractions among which the Lattice
Boltzmann Method, that is eventually a Cellular Automata-based
paradigm. Nevertheless, it currently does not allow for parallel
execution on distributed memory systems, which can be a great
limitation in some cases. Eventually, CAMELot offers an integrated
simulation environment for XCA development and allows for parallel
execution on both shared and distributed memory systems thanks to
the message passing paradigm, not permitting however the
exploitation of modern many-core devices.


This chapter is devoted to the description of \texttt{OpenCAL} which aims to be a portable parallel computing abstraction layer for scientific computing.
It provides the Extended Cellular Automata general formalism as a Domain
Specific Language, allowing for the straightforward parallel
implementation of a wide range of complex systems. Cellular
Automata, Finite Differences and, in general, other structured
grid-based methods are therefore supported. Different versions of
the library allow to exploit both multi- and many-core shared
memory devices, as well as distributed memory systems. Specifically,
OpenMP- and OpenCL-based implementations have been developed, both
of them providing optimized data structures and algorithms to
speed-up the execution and allowing for a transparent parallelism to
the user. A MPI-based implementation is also currently under
development and allows to exploit many-core accelerators on
interconnected systems. With respect to the above cited softwares,
OpenCAL therefore provides both the higher CAMELot modeling approach
and, similarly to OP2, allows for the execution on a wide range of
shared and distributed parallel platforms (even if by adopting a
classic library approach). In addition, OpenCAL provides different
embedded strategies and optimization algorithms which allow to
progressively improve the computational performance of different
kinds of models and simulations.
\texttt{OpenCAL} is released under Lesser GNU Public License (LGPL) version 3 and is freely down-loadable from GitHub at the following link: \url{https://github.com/OpenCALTeam/opencal}. \texttt{OpenCAL} allows for  the definition of computational models based on CA, XCA and FDM. It is designed to be easily extended and applied to all computational methods based on regular and uniform grids. 
The implementation described in this chapter targets shared multi-core, distributed memory and GPUs and is designed to make the parallelism transparent to the user addressing and hiding many aspects of the underlying formal computational model and parallel execution platform. 

In the following, the OpenCAL architecture is presented and the
OpenMP- and OpenCL-based parallel implementations described.  The implementation of a first simple example of
application for multi- and many-core devices is also
presented and discussed in order to show how straightforward the OpenCAL-based model development is. 
Therefore, the $SciddicaT$ XCA landslide simulation model
\cite{avolio2000simulation} is then considered as a more complex reference example for correctness and computational performance evaluation on multi-core
CPUs, many-core GPUs. In particular, three different versions of
$SciddicaT$ are refeered to in this chapter, which progressively exploit OpenCAL built-in features and, for each of them, different implementations based on the serial and parallel versions of the library are proposed. Eventually, results of a further study performed to devise the best platform for
execution, depending on the model's computational intensity and the
domain extent, is presented. A general discussion concerning OpenCAL
and future outcomes concludes the chapter.


\section[Software Architecture]{\texttt{OpenCAL} Software Architecture and the Considered Parallel Computing Paradigms}

The OpenCAL architecture is depicted in Figure
\ref{fig:architecture}. At the higher level of abstraction, the
Scientist conceptually designs the computational model, by referring
the Extended Cellular Automata general formalism. Structured
grid-based models whose evolution is determined by local rules, as
well as by global laws or even by a combination of local and global
operations, are therefore fully supported. At this level, domain
topology and extent, boundary conditions, substates (each of them
representing the set of admissible values of a given characteristic
assumed to be relevant for the modeled system and its evolution),
neighborhood (defining the pattern over which local rules are
applied), and elementary processes (defining the local rules of
evolution), are formalized. The simulation process is also designed
at this level, by specifying the initial conditions of the system,
optional global operations (e.g. steering or global reductions), and
a termination criterion to stop the system evolution. 
Boundary conditions can be implemented by the user by creating ad-hoc code within the transition function which treats the boundary cells (which can be identified to belonging to the boundary, within the code) s.t. the boundary condition is enforced.
For instance if implementing a heat transfer model, adiabatic walls can be enforced programmatically by setting heat transfer to $0$ only for those cells that make up the walls. Any other boundary condition can be implemented similarly.
Note that, being supported by OpenCAL, some specific optimizations can be
accounted at design time. Specifically, the explicit updating
feature allows to both redefine the elementary processes application
order and to selectively update substates after the application of
each elementary process, while the \emph{active cells optimization},
also known as \emph{quantization}, allows to restrict the
computation to a subset of the whole computational domain, by
excluding stationary cells.
\begin{figure}
	\begin{center}
\includegraphics[width=1.0\textwidth]{./images/opencal/Figure05}
\caption[\texttt{OpenCAL} architecture.]{\texttt{OpenCAL} architecture. At the higher level of abstraction, the models,
	as well as the simulation process, and possible optimizations are
	designed. \texttt{OpenCAL} can be found at the implementation abstraction
	layer, allowing for a straightforward implementation of the designed
	model. In fact, it can be considered as a domain-specific language for
	the CA, XCA and FDM computational methods, built on top of the C
	language and the \texttt{OpenMP} and OpenCL APIs. \texttt{OpenCAL} applications can be
	executed at the hardware level on both multi-core CPUs and many-core
	devices, while the execution on cluster is planned but currently not
	supported.}
\label{fig:architecture}
\end{center}
\end{figure}

\texttt{OpenCAL} can be found in the implementation abstraction level. As it
can be seen, four different versions can be considered for
implementing the previously designed computational model, namely
\texttt{OpenCAL}, \texttt{OpenCAL-OMP} and \texttt{OpenCAL-CL} and \texttt{OpenCAL-MPI}. The first one refers to the  serial implementation of the library, while second and the third are \texttt{OpenMP}  and \texttt{OpenCL}-based parallelizations, respectively, as pointed out by
the language/low-level library level. The fourth one is a cluster ready implementation that allows to execution on distributed memory cluster where each node can exploit multiple GPUs. All implementations are written in C for the maximum efficiency and provide high-level data types and functions that match the XCA formal components, allowing  for a straightforward implementation of the designed computational  model, by also allowing to ignore low-level issues like memory  management and I/O operations. In this respect, \texttt{OpenCAL} can be  considered as a domain-specific language (DSL) for the CA, XCA and  FDM computational methods. Finally, at the hardware level, depending  on the adopted version of the library, execution can be performed on  single- and multi-core CPUs, or on many-core accelerators like GPUs,  almost transparently to the user. Figure \ref{fig:architecture} also shows hybrid MPI+Open-MP and MPI+OpenCL parallel implementations of \texttt{OpenCAL}, which will allow to exploit clusters of CPUs and many-core accelerators (see Chapter \ref{ch:opencal_cluster}). 

Note that an \texttt{OpenMP}-based parallelization is generally more
straightforward with respect to one based on OpenCL or MPI and, when
compilers will fully support the 4.0/4.5 \texttt{OpenMP} specifications, it
will be possible to execute \texttt{OpenMP}-based applications on both
multi-core CPUs and many-core high-performance devices. On the other
hand, an OpenCL-based parallelization allows to exploit a wide range
of high-performance many-core devices straight away and, with greater control on the underlying hardware and on the execution flow, allowing better exploitation of the hardware capabilities. 
 For these reasons, both the \texttt{OpenMP} and OpenCL versions of \texttt{OpenCAL} have been developed and are maintained. 

\section{The Open Computing Abstraction Layer}
\label{sec:OpenCAL}
The OpenCAL API was designed to be clear and easy to use. For this
purpose, it follows some naming conventions, the most important of
which are listed below:
\begin{itemize}
	\item \verb'CALbyte', \verb'CALint', and \verb'CALreal' redefine
	the \verb'char', \verb'int' and \verb'double' C native scalar
	types, respectively;
	\item Derived data types start with the \verb'CAL' prefix (or
	\verb'CALCL' for some specifc OpenCAL-CL data types), followed
	by a type identifier formed by one or more capitalised keywords,
	an optional suffix identifying the model dimension
	(e.g. \verb'2D' or \verb'3D'), and an eventual optional suffix
	specifying the basic scalar type, which can be \verb'b',
	\verb'i', or \verb'r', for \verb'CALbyte', \verb'CALint' and
	\verb'CALreal' derived types, respectivey
	(e.g. \verb'CALSubstate3Dr' represents an example of
	three-dimensional double precision-based data type - cf. below);
	\item Constants and enumerals start with the \verb'CAL_' prefix,
	followed by one or more uppercase keywords separated by the
	\verb'_' character (e.g. the \verb'CAL_TRUE' and
	\verb'CAL_FALSE' boolean enumerals);
	\item Functions are characterised by the \verb'cal' prefix (or
	\verb'calcl' for some specifc OpenCAL-CL functions), followed by at
	least one capitalized keyword, and end with a suffix specifying
	the model dimension and the basic datatype
	(e.g. \verb'calSet2Di' represents an example of an API function
	acting on a bi-dimensional integer based data type).
\end{itemize}
In the following, the \verb'{arg1|arg2|...|argn}' and
\verb'[arg1|arg2|...|argn]' conventions will be adopted: the first
one identifies a list of $n$ mutually exclusive arguments, where one
of the arguments is needed; the second is used to identify a set of
$n$ non-mutually exclusive optional arguments. As an example,
\verb'calGet[X]{2D|3D}{b|i|r}()' function actually identifies a set
API functions with one optional and two mandatory suffixes: the
first one, if present, indicates that the fuction is able to access
naighborhood data (\verb'X' is the symbol commonly used in the XCA
formalism to refer the neighborhood), while the last two ones
indicate the domain dimension and the basic type of the data to be
accessed, respectively.

Among derived data types, \verb'CALParameter{b|i|r}' represents an
alias for the corresponding basic \texttt{OpenCAL} scalar data type, and can
be optionally used for defining model parameters.


\subsection{API}
An \texttt{OpenCAL} model is declared as a pointer to \verb'CALModel{2D|3D}'
and defined by means of the \verb'calCADef{2D|3D}()' function. The
model object stores the dimensions of the computational domain in
terms of number of rows and columns (and also slices in case of a 3D
model), the computational domain boundary topology (e.g., if a 2D
computational domain has to be considered as a bounded or an
unbounded torus), the neighbourhood pattern, and also registers
pointers to substates and elementary processes composing the
transition function. Moreover, it manages a sub-structure which
allows to exploit the built-in quantization feature by means of
which, based on user-specified criteria, the computation can be
restricted to a subset of non-stationary cells (also called active
cells) of the whole computational domain.

 As regards neighborhoods, \texttt{OpenCAL} provides a set of predefined
patterns. For instance, the \verb'CAL_MOORE_NEIGHBORHOOD_{2D|3D}'
enumeral refers to the Moore pattern (cf. Figures
\ref{fig:2Dneighborhood}b and \ref{fig:3Dneighborhood}b). von
Neumann 2D and 3D neighborhoods are also predefined, as well as 2D
hexagonal patterns (cf. Figures \ref{fig:2Dneighborhood}c and
\ref{fig:2Dneighborhood}d). Custom neighborhoods can also be defined
in \texttt{OpenCAL} by considering the \verb'CAL_CUSTOM_NEIGHBORHOOD_{2D|3D}'
enumeral and the \verb'calAddNeighbor{2D|3D}()' function, which adds
a cell to the neighbourhood by means of its relative coordinates
with respect to the central one. Note that, a zero-based index is
assigned to the neigbouring cells in order to address them without
the need to provide their relative coordinates, as shown in Figures
\ref{fig:2Dneighborhood} and \ref{fig:3Dneighborhood}.
\begin{figure}[!htbp]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./images/opencal/Figure01}
		\caption[Examples of \textit{Von Neumann} and \textit{Moore} neighborhoods.]{Examples of \textit{Von Neumann} (a) and \textit{Moore} (b) neighborhoods for two-dimensional CA with square cells. Examples of Moore neighborhoods are also shown for hexagonal CA, both for the cases of horizontal (c) and vertical (d) orientations. Central cell is represented in dark gray, while adjacent cells are in light gray. A reference system is here considered to evaluate cells coordinates in terms of row and column indices in a matrix-style representation, and a $0$-based numerical identifier assigned to each cell in the neighborhood for straightforward access.}
		\label{fig:2Dneighborhood}
	\end{center}
\end{figure}
Elementary processes, both local interactions and internal
transformations, are defined by means of callback functions and
registered to a computational model by means of the
\verb'calAddElementaryProcess{2D|3D}()' function. Each elementary
process callback must return \verb'void' and takes a list of integer
arguments, representing the coordinates of a generic cell in the
computational domain. Elementary processes define the \texttt{OpenCAL}
transition function, and can be implicitly applied by the simulation
loop to the cells of the computational domain in the same order in
which they were registered to the computational model, or in a user
defined order (cf. below).


\begin{figure}[!htbp]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./images/opencal/Figure04}
		\caption[Examples of \textit{Von Neumann} and \textit{Moore} neighborhoods for three-dimensional CA with cubic cells.]{Examples of \textit{Von Neumann} (a) and \textit{Moore} (b, b') neighborhoods for three-dimensional CA with cubic cells. Central cell is represented in dark gray, while adjacent cells are in light gray. A reference system is here considered to evaluate cells coordinates in terms of row,
		column and slice indices in a matrix-style representation, and a
		0-based numerical identifier assigned to each cell in the neighborhood
		for straightforward access.}
		\label{fig:3Dneighborhood}
	\end{center}
\end{figure}
Substates are defined as pointers to
\verb'CALSubstate{2D|3D}{b|i|r}' and can be registered by means of
the \verb'calAddSubstate{2D|3D}{b|i|r}()' function. Substates are
internally defined by means of two linearized arrays (also called
computational layers), having the same dimensions of the
computational domain. The \emph{current} layer is used as a
read-only memory for retrieving central and neighboring cells
current states, while the \emph{next} one for updating the new value
for the central cell. This is a commonly adopted solution for
obtaining the \emph{implicit parallelism}, thanks to which cells
appear to be simultaneously updated with respect to each other, even
in the case of serial computation. Single layer substates can be
also defined in \texttt{OpenCAL} by simply registering them through the
\verb'calAddSingleLayerSubstate{2D|3D}{b|i|r}()' function. In this
case, they only consist of the current computational layer, and can
be used for internal transformations processing. To retrieve the
current value of a substate for a (central) cell by providing its
coordinates within the computational domain, the
\verb'calGet{2D|3D}()' function can be adopted, while the
\verb'calGetX{2D|3D}()' function can be considered for obtaining the
same information for a neighbouring cell, by providing an additional
parameter specifying the index of the cell in the neighborhood
(cf. Figures \ref{fig:2Dneighborhood} and \ref{fig:3Dneighborhood}
for predefined neighborhoods). Eventually, the
\verb'calSet{2D|3D}()' function can be used to set the new value of
a substate for the (central) cell to the next computational
layer. In the case of a single layer substate, the
\verb'calSetCurrent{2D|3D}{b|i|r}()' function has to be employed for
updating purposes. It acts like the \verb'calSet{2D|3D}()' function,
with the exception that the new value is written on the current
computational layer. Note that, after the application of each
elementary process, all the registered substates are implicitly
updated, i.e. the next layer is copied into the current
one. However, this behaviour can be overridden and substates
explicitly, as well as selectively, updated by means of the
\verb'calUpdateSubstate{2D|3D}{b|i|r}' function
(cf. below). Obviously, single layer substates do not need to be
updated.


\subsection{The quantization strategy}
In many grid-based simulations, system's dynamics only affects a
small region of the whole computational domain. For instance, this
is the case of topologically connected phenomena, like debris or
lava flows. In these cases, a naive approach where the overall
domain is processed can lead to a considerable waste of
computational resources, even in the case stationary cells
(i.e. those cells that do not change their state to the next
computational step) are only checked and the application of the
evolution rules skipped.

Different approaches have been proposed to improve the efficiency of
the naive approach. Among them, the hyper-rectangular bounding box
(HRBB) optimization, consisting in surrounding the simulated
phenomenon by means of a fitting rectangle (or a parallelepiped, in
the case of a 3D model), by contextually restricting the computation
to this specific sub-region, proved to be a simple but effectiveg
approach in different cases (see e.g. the work of \citeauthor{D'Ambrosio2013630} \cite{D'Ambrosio2013630}). However, HRBB demonstrated its limit
in the simulation of scattered phenomena, where the hyper-rectangle
can easily grow up to the whole domain, by however embedding a
considerable number of inactive cells.

A more effective approach, which is also able to optimally
distribute the computational load in case of parallel execution,
consists in maintaining a dynamic set of coordinates of the only
active cells during the simulation, by restricting the computation
only to this set (see e.g. \cite{DiGregorio20131183}). The
activation state for a cell generally depends on the specific system
to be simulated. In many cases, e.g. in computational
fluid-dynamics, a threshold-based criterion can be adopted. For this
reason, this latter approach is commonly known as quantization. Even
if more complex to be implemented, in many cases it outperforms the
HRBB approach and, for this reason, it was chosen over HRBB in OpenCAL.

The \texttt{OpenCAL} quantization feature\footnote{The term quantization has been chosen to refer to this optimization as when enabled, it treats the grid space as a group of quanta of computation instead of as a single massive computation.} is implemented by considering a
dynamic array of active cells, $A$ of the size of the grid-space, which is initially empty. An
array of flags, $F$, having the same dimension of the computational
domain is also considered, which is initially set to
\verb'CAL_FALSE' in each position. Eventually, an integer variable,
$size$, initially set to zero, is used to evaluate the new dimension
of $A$ per effect of the add/remove operations. In order to add a
cell to $A$, the \verb'calAddActiveCell{2D|3D}()' function can be
used, which sets the flag value to \verb'CAL_TRUE' in the
corresponding position of the array $F$ and increases $size$ by
one. Similarly, the \verb'calAddActiveCellX{2D|3D}()' function adds
a neighbouring cell to $A$. Eventually, the
\verb'calRemoveActiveCell{2D|3D}()' function can be used to remove a
cell from $A$, by contextually modifying the corresponding flag in
$F$ and decrementing $size$ by one. When an add/remove stage is
completed, e.g. after the execution of an elementary process, and
the (scattered) array $F$ is well defined, the set $A$ must be
updated. The update process deletes the current set $A$, allocates a
new set of active cells of dimension $size$, and applies a
straightforward serial stream compaction algorithm by processing the
entire array of flags $F$, as shown in Figure
\ref{fig:active_cells}. As evident, the algorithm has a $O(n)$
computational complexity, being $n$ the number of cells of the
cellular space $R$. In this way, only the loop updating $A$ occurs
on the whole computational domain (since the $F$ array must be fully
checked). In fact, when $A$ is not empty, it is processed instead of
the whole computational domain and both elementary processes
computation and substates updating take place on the active
cells. As substates, even $A$ is implicitly updated at the end of
each elementary process. 

Using the quantization optimization is quite
straightforward. Firstly, it must be enabled at model object
definition time by means of the \verb'calCADef{2D|3D}()'
function. Subsequently, the \verb'calAddActiveCell[X]{2D|3D}()'
function can be used to mark the central cell and its neighbors (if
the \verb'X' version of the function is considered) to be added to
$A$, while the \verb'calRemoveActiveCell{2D|3D}()' to mark the
central cell to be removed. All these functions essentially write a
8-bit long Boolean value to $F$ and, for this reason, there is not
any risk to obtain a corrupted value, even in the case of parallel
execution (i.e. in the case two threads/work-items attempt to store
their own value to the same memory word at the same time). Even in
the case of OpenCAL-CL, if the same instruction is executed by more
than one work-item (even belonging to different work-groups) to the
same location in global memory (where $F$ is stored), the access is
serialized and at least one access is guarantied (even if which the
actual thread performing the operation is undefined -
cf. e.g. \cite{CUDA}). Eventually, in case of explicit update
scheme, the \verb'calUpdateActiveCells{2D|3D}' function must be
explicitly invoked to update $A$ after each add/remove phase is
complete.

Note that, since the API allows to modify the neighboring cells
activation state, the quantization optimization can give rise to
race conditions. Nevertheless, to avoid them it is sufficient to
keep the add and remove phases disjoint, i.e. performed by different
elementary processes. In fact, if the same elementary process could
both add and remove cells to/from $A$, two different (central) cells
could update the same (neighboring) cell to different activation
states, and the resulting value in $F$ before the stream compaction
execution would depend on the application order of the elementary
process to the cells.

There are other ways to implementation the quantization strategy s.t. less time or space space is used and some of them have also been investigated as for instance a strategy that allows to use space proportional to the number of active cells, which consist of a dynamic list of active cells. The one shipped with this release of OpenCAL guarantees uniform performance on all platforms and is for this reason adopted. future versions will feature multiple implementation from which the user can choose from.


%However, active cells update can also be
%explicitly performed by means of the
%\verb'calUpdateActiveCells{2D|3D}' function (cf. below).
%Eventually, note that the add and remove stages updating the values
%in $F$ have to be performed separately, e.g. by two different
%elementary processes. In fact, if the same elementary process could
%both add and remove cells depending for instance on the current
%state of the central cell, two different cells could update the same
%(neighbouring) cell to different activation states, and the
%resulting value in $F$ would depend on the (serial) application
%order of the elementary process, giving rise to a possible logical
%error.

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./images/opencal/Figure06}
		\caption[The adopted serial stream compaction algorithm.]{An example of application of the serial stream compaction algorithm
			adopted to update the set $A$ of active cells in the serial
			implementation of \texttt{OpenCAL}. In the example, active cells are
			represented in gray within a two-dimensional 4x4 matrix of flags,
			implemented as a linearized array, $F$. The stream compaction
			algorithm simply processes $F$ and produces the compacted array $A$ as
			output, containing the coordinates of the active cells. The global
			state transition is therefore limited to the cell in $A$, and a new
			configuration of the system obtained. The process is therefore
			repeated at the next computational step.}
		\label{fig:active_cells}
	\end{center}
\end{figure}

In order to perform a simulation, a pointer to \verb'CALRun{2D|3D}'
must be declared and defined by means of the
\verb'calRunDef{2D|3D}()' function. The simulation object stores a
pointer to the \texttt{OpenCAL} model to be run, a step counter, the initial
and final computational steps, an enumeral of type
\verb'CALUpdateMode', which specifies the substates update policy
(if implicit or explicit), and registers a set of four optional
callback functions. These latter, described below, take a pointer to
an \texttt{OpenCAL} model as argument and do not return any value, with the
exception of the termination function, which returns a
\verb'CALbyte' (\verb'CAL_TRUE' if the termination criterion is
satisfied, \verb'CAL_FALSE' in the other case). The optional
callback functions are:
\begin{itemize}
	\item \verb'init()': It can be used to set the initial conditions of
	an \texttt{OpenCAL} computational model. It can be registered to the
	simulation object by using the \verb'calRunAddInitFunc{2D|3D}()'
	function. If defined, the \verb'init()' function is executed once
	before the simulation loop.
	\item \verb'globalTransition()': It can be used to redefine the
	execution order of the registered elementary processes and to
	perform selective substate updating.\hfil \\The \texttt{calApplyElementaryProcess{2D|3D}()} function can be used
	within the registered callback to apply a registered elementary
	process to each cell of the computational domain, while the
	\verb'calUpdateSubstate{2D|3D}{b|i|r}' to update a registered
	substate. The \verb'globalTransition()' function also allows to
	call functions that can perform global operations over the
	computational domain, e.g. reductions. It can be registered to the
	simulation object by using the
	\verb'calRunAddGlobalTransitionFunc{2D|3D}()' function. If
	defined, the \verb'globalTransition()' function overrides, i.e. is
	applied instead of, the implicit global transition function.
	\item \verb'steering()': It can be used to perform global
	operations, e.g. reductions, at the end of each computational
	step, that is after that all the elementary processes have been
	applied. Predefined reductions allow to compute global minimum,
	maximum, sum, product, as well as logical and bit-wise AND, OR and
	NOT operations on the registered substates. A steering can be
	registered to the simulation object by using the
	\verb'calRunAddSteeringFunc{2D|3D}()' function. If defined, the
	\verb'steering()' function is applied at the end of each
	computational step.
	\item \verb'stopCondition()': It can be used to define a stopping
	criterion for the simulation. Note that, if the last computational
	step was set to \verb'CAL_RUN_LOOP', the \verb'stopCondition()'
	callback is mandatory to stop the simulation. It returns
	\verb'CAL_TRUE' if the termination criterion is satisfied,
	\verb'CAL_FALSE' in the other case. The termination callback can
	be registered by using the
	\verb'calRunAddStopConditionFunc{2D|3D}()' function. If defined,
	the \verb'stopCondition()' function is executed at the end of each
	computational step.
\end{itemize}
The simulation process can be executed by means of the
\verb'calRun{2D|3D}' function. Algorithm \ref{algo:simuloop}
outlines the \texttt{OpenCAL} implicit simulation process, which takes place
in the case the enumeral \verb'CAL_UPDATE_IMPLICIT' was used as last
argument for the \verb'calRunDef{2D|3D}()' function. The
\verb'init()' function, if defined, is called first and then active
cells (if quantization is enabled) and substates are
updated. Moreover, the \verb'step' counter and the \verb'halt'
variable, that is used to check the simulation termination
condition, are set to the initial step and to \verb'CAL_FALSE',
respectively. The main simulation loop follows, which applies
elementary processes in the order they were registered to each cell
of the computational domain. After the execution of each elementary
process, active cells and substates are updated. If defined, the
\verb'steering()' global function is therefore called and active
cells and substates updated. The \verb'stopCondition()' function is
also called and the step counter increased. The simulation loop
continues while the \verb'halt' variable, whose value is set by the
\verb'stopCondition()' function, is \verb'CAL_FALSE' or the final
step of computation is met.

\begin{algorithm}
	\DontPrintSemicolon \SetKwFunction{Init}{init()}
	\SetKwFunction{Update}{update}
	\SetKwFunction{Steering}{steering()}
	\SetKwFunction{Stop}{stopCondition()}
	\SetKwFunction{Finalize}{finalize()}
	
	\Init \tcp*{Call the \Init global function}
	 \If{quantization}{
		\Update($A$) \tcp*{Update the array of active cells}
	 }
	\ForAll{$q \in Q$}
		{ \Update($q$) \tcp*{Update the substate $q$}
		}
	$step \gets initial\_step$\; $halt \gets false$\;
	\While{$\neg$halt $\land$ (step $\leq$ final\_step $\vee$
		final\_step = CAL\_RUN\_LOOP)}{
		 \ForAll{$e$ of $\sigma$}{
			\ForAll{$(A \neq \emptyset \land i \in A) \vee$ $i \in R$}
				{
				$e(i)$ \tcp*{Apply the elementary process $e$ to the cell
					$i$}
				}
			\If{quantization}{
				 \Update($A$) \tcp*{Update the array of active cells} 
			}
			\ForAll{$q \in Q$}{ 
				\Update($q$) \tcp*{Update the substate $q$}
			 }
		  }
	  	\Steering \tcp*{Call the \Steering global function}
	  	 \If{quantization}{
	  	 	 \Update($A$) \tcp*{Update the array of active cells}
	 	  } 
 	  	\ForAll{$q \in Q$}{
			\Update($q$) \tcp*{Update the substate $q$}
		 }
		 $halt \gets $\Stop \tcp*{Check the stop condition} $step \gets step + 1$\;
	 }
	\Return\;
	\caption{{\texttt{OpenCAL} main implicit simulation process.}}
	\label{algo:simuloop}
\end{algorithm}

Explicit update can be set by using the \verb'CAL_UPDATE_EXPLICIT' enumeral as last argument of the \verb'calRunDef{2D|3D}()' function. In this case, the global transition function must be overridden and both active cells and substates explicitly updated. Eventually, instead of the \verb'calRun{2D|3D}()', which executes the whole simulation process, the
\verb'calRunCAStep{2D|3D}()' function can be used to apply a single
step of the global transition function, including steering and stop
condition checking. In this case, the initialization function must
be called explicitly, as well as the simulation counter increased.




\subsection{\texttt{OpenCAL} Conway's Game of Life}
As a first illustrative example, we here present the \texttt{OpenCAL}
implementation of the Turing complete Conway's Game of Life\cite{gardner1970a}, one of the most simple, yet powerful examples of CA, devised by mathematician John Horton Conway in 1970. See Section \ref{sect:GOL} at page \pageref{sect:GOL} for a formal definition of \textit{The Game of Life} and a  description of its governing rules.

The program in Listing \ref{lst:cal_life} provides a complete
\texttt{OpenCAL} implementation of Game of Life in just few lines of code,
by defining both the CA model and the simulation object, needed to
let the CA evolve step by step.

%%%% GOL listing C serial implementation %%%%
\lstset{language=[OpenCL]C,
	caption=An \texttt{OpenCAL} implementation of the Conway's Game of
	Life., 
	label=lst:cal_life, 
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\lstinputlisting{./code/cal_life.c}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\texttt{OpenCAL} header files are included at lines 1-3. Specifically,
\verb'cal2D.h' allows to define 2D CA and substates,
\verb'cal2DRun.h' the simulation object, while \verb'cal2DIO.h'
provides some basic I/O functions. The CA object is declared at
line 6, while lines 7 and 8 declare a substate and a simulation
object, respectively.  Objects declared at lines 6-8 are defined
later in the \verb'main' function. In particular, the \verb'life'
CA object is defined at line 24 by the \verb'calCADef2D()'
function. The first 2 parameters define the dimensions of the
computational domain (in terms of number of rows and columns,
respectively), while the third the Moore
neighborhood. Furthermore, the fourth parameter sets a toroidal
topology for the cellular space, while the last switches the
active cells optimization to off.

The CA simulation object is defined at line 25 by the
\verb'calRunDef2D()' function, where the first parameter is a
pointer to the \verb'life' CA object, while the second and third
parameters specify the initial and last simulation steps,
respectively. Eventually, the last parameter sets the update
policy to implicit.

Line 27 allocates memory and registers the integer-based $Q$
substate to the CA by means of the \verb'calAddSubstate2Di()'
function, while line 29 registers an elementary process by means
of the \verb'calAddElementaryProcess2D()' function. Here, the
\verb'lifeTransitionFunction' parameter represents the name of a
developer-defined function implementing the transition function
rules (cf. lines 10-20). Within the elementary process, the
\verb'calGet[X]2Di()' and \verb'calSet2Di()' functions are used
for reading and updating purposes, respectively. The
\verb'calInitSubstate2Di()' function at line 31 sets the whole $Q$
substate to the value 0 (for both the current and next
layers). Lines 32-36 define a so called \emph{glider} pattern
(cf. Figure \ref{fig:cal_life_glut}a) by means of the
\verb'calInit2Di()' function. The \verb'calSaveSubstate2Di()'
function at line 38 saves the $Q$ substate to file, while the
\verb'calRun2D()' function at line 40 enters the simulation
process (actually, only one computational step in this example),
and returns to the \verb'main' function when the simulation is
terminated. The \verb'calSaveSubstate2Di()' is called again at
line 42 to save the new (last) configuration of the CA, while the
last two functions at lines 44 and 45 release memory previously
automatically allocated by \texttt{OpenCAL} for the CA, substates
(actually, only $Q$ in this case) and simulation object. The
\verb'return' statement at line 47 ends the program.

Figure \ref{fig:cal_life_glut} show a graphical representation of
the initial and final configurations of Game of Life,
respectively, as implemented in Listing \ref{lst:cal_life}. As
expected, the glider initially defined has evolved into the new
correct configuration.

  \begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./images/opencal/Figure04_new}
		\caption{Graphical representation of one computational step of
			the Game of Life, showing the (a) initial and (b) final
			configurations of the system. Alive cells are represented in
			grey, dead cells in white.}
		\label{fig:cal_life_glut}
	\end{center}
\end{figure}

\section{The \texttt{OpenCAL} \texttt{OpenMP}-based Parallel Implementation}
\label{sec:OpenCAL-OMP}

In this section we present \texttt{OpenCAL-OMP}, the \texttt{OpenMP}-based parallel
implementation of \texttt{OpenCAL}, which allows for seamless parallel execution on
shared memory computing systems in a SIMD fashion. For brevity, we
only present and discuss differences with respect to \texttt{OpenCAL} in
terms of API and internal algorithms.
Since one of the main goal is to obtain transparent parallelism, the \texttt{OpenCAL-OMP} API has been designed to differ as less as possible to the serial
one, leading to the adoption of the same naming conventions, interface,
and programming model.

Double layer substates were also considered in \texttt{OpenCAL-OMP}, which permitted a straightforward lock-free \texttt{OpenMP} parallelization. In fact no race conditions can occur, since the current layer is accessed in read mode, and the update phase access to the next layer is limited to the memory location associated with
the central cell. As a consequence, elementary processes and
substates updating loops, as well as global reduction operations,
were parallelized by simply considering lock-free \texttt{OpenMP} pool of
threads, as shown in Figure \ref{fig:pool_of_threads}. In the
example, a pool of three threads were used to partition the
computational domain in three subregions, which were therefore
processed in parallel both during the application of an elementary
process and the subsequent substates updating. Note that, in the
case only a subset of cells are actually involved in computation, as
in the example, load unbalance conditions can occur. In fact, the
third thread is wasted, since it only applies the elementary process
on a subset of stationary cells. A dynamic \texttt{OpenMP} scheduling is
adopted in this case to mitigate the unbalance among chunks.
\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./images/opencal/Figure08.pdf}
		\caption[An example of \texttt{OpenCAL-OMP} parallel application of an elementary process to a substate $Q$ and its subsequent parallel updating.]{An example of \texttt{OpenCAL-OMP} parallel application of an elementary process to a substate $Q$ and its subsequent parallel updating. The
			computational domain is initially partitioned by means of a pool of
			three threads (fork phase). These latter concurrently apply the
			elementary process by reading state values from the current layer and by updating new values to the next one. At the end of the elementary process application, threads implicitly synchronize by joining into the master one (join phase), and the parallel update phase starts. As before, a pool of threads concurrently copies the next layer into the current one and the new configuration of $Q$ is obtained. A join phase eventually occurs, which ensures data consistency before the application of another elementary process.}
		\label{fig:pool_of_threads}
	\end{center}
\end{figure}
As regards the quantization feature, it is still based on the
dynamic array of active cells $A$, containing the coordinates of non
stationary cells, and on the array of flags $F$, having the same
dimension of the computational domain, used to register the cells
activation state during the application of the transition
function. $F$ is initially set to \verb'CAL_FALSE' in each position
and, each time a cell has to be added to/removed from $A$, the
corresponding position in $F$ is updated by a \verb'CAL_TRUE'/\verb'CAL_FALSE' value. 
At the end of the add/remove
stage, a lock-free parallel stream compaction is executed on the
resulting scattered array $F$ to obtain the new set of active cells
$A$. For this purpose, $F$ is partitioned over the $N$ running
threads. Each of them builds a private subset, $A_p$ $(p=0, 1,
\ldots N-1)$, of cells to be added to/removed from $A$, by
contextually evaluating its relative size, $size_p$. Eventually, the
actual $size$ of $A$ is obtained as $size = \sum_{p=0}^{N-1}
size_p$, and the subsets $A_p$ assembled together to form the new
set $A$, as shown in Figure \ref{fig:active_cells_omp}. As for the
case of the \texttt{OpenCAL} serial stream compaction, it is evident that
also in this case the algorithm has a $O(n)$ computational
efficiency, where $n$ is the number of cells of the cellular
space. Note that, being both the \verb'CAL_TRUE' and \verb'CAL_FASE'
8-bit long enumerals, no inconsistent values can be obtained even in
the case more than one thread accesses the same location in $F$
simultaneously and, therefore, a lock-free updating policy was also
here considered for the quantization optimization. However, as for
\texttt{OpenCAL}, the add and remove stages updating the values in $F$ have
to be performed separately, e.g. by two different elementary
processes, in order to avoid possible race conditions. In fact, if
the same elementary process could both add and remove cells
(depending for instance on the current state of the central cell),
two different threads applying the elementary process to two
different cells could update the same (neighbouring) cell to
different activation states. In this case, the resulting value in
$F$ would depend on which thread writes the value for last, giving
rise to a possible logical error. Eventually, note that the
quantization feature is able to optimally distribute the
computational load over the running threads, since stationary cells
are simply excluded by the computation (cf. Figure
\ref{fig:active_cells_omp}). A more efficient static \texttt{OpenMP}
scheduling is therefore here adopted instead of the dynamic one.

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./images/opencal/Figure09}
		\caption[OpenMP stream compatcion algorithm adopted in OpenCAL.]{An example of application of the parallel stream compaction algorithm used in \texttt{OpenCAL-OMP} to update the set $A$ of active cells. Active cells are represented in gray within a two-dimensional 4x4 matrix of flags, implemented as a linearized array, $F$. The parallel stream
			compaction algorithm processes $F$ by means of a lock-free pool of
			threads, resulting in the $A_p$ $(p=0,1,2)$ partial arrays of active
			cells. These latter are eventually assembled together by the master
			thread, resulting in the compacted array $A$. A new pool of threads
			therefore applies the state transition function in parallel on $A$
			with an optimal load balancing, and a new configuration of the system
			is obtained. The process is therefore repeated at the next
			computational step.}
		\label{fig:active_cells_omp}
	\end{center}
\end{figure}


\subsection{\texttt{OpenCAL-OMP} implementation of the Game of Life}
Conway's Game of Life can be straightforwardly implemented in
parallel by using \texttt{OpenCAL-OMP}. The source code is almost identical
to the one in Listing \ref{lst:cal_life}, with the exception of
the first three lines, where the \texttt{OpenCAL-OMP} header files are
included instead of the OpenCAL ones. For this purpose, it is
sufficient to change the headers parent directory from
\verb'OpenCAL' to \verb'OpenCAL-OMP'. For instance, the
\verb'OpenCAL/cal2D.h' header is replaced by
\verb'OpenCAL-OMP/cal2D.h'. The remaining lines of code are
unchanged and therefore source code is omitted. As for the case of
the \texttt{OpenCAL} implementation, Figure \ref{fig:cal_life_glut} shows
the initial and final configuration of the system.


\section{The \texttt{OpenCAL} OpenCL-based Parallel Implementation}
\label{sec:OpenCAL-CL}
In this section, we present the OpenCL-based parallel implementation
of OpenCAL, which allows for the parallel execution on both shared
memory computing systems and many-core acceleration devices in a
SIMD fashion, by highlighting the differences with respect to the
serial and OpenMP implementations of OpenCAL in terms of API and
internal algorithms.

The API is very similar to the serial one and adopts the same
programming model and naming conventions, with the exception that
the \verb'calcl', \verb'CALCL' and \verb'CALCL_' prefixes are
adopted for functions, data types and constants, respectively. The
main difference with respect to OpenCAL and OpenCAL-OMP is that an
OpenCAL-CL application is subdivided in two parts, one running on
the CPU, the other in parallel on a compliant device. The host
application defines the host-side computational model, registers the
required substates to it, while elementary processes and other
global function are implemented as OpenCL kernels and registered to
a device-side computational model. When a device-side model is
defined, data registered to the host-side model is implicitly copied
to the compliant device global memory, transparently to the
user. Within kernels, however, the user can transfer data to the
local memory and then update the global memory when the local
operations are complete, for better performance. Data stored in
global memory is therefore copied back to the host at the end of
the simulation process. This latter if equivalent to that of OpenCAL
and OpenCAL-OMP (cf. Algorithm \ref{algo:simuloop}), with the
exception that kernels are executed device-side and the simulation
process can not be currently, in this first OpenCAL release, made explicit, because the \textit{explicit} API is not entirely correctly implemented.

Grid of work-items can be two- or three-dimensional, depending on
the dimension of computational model, if 2D or 3D, respectively. In
the case the quantization feature is exploited, a one-dimensional
grid is considered. The number of work-items is evaluated for each
model dimension by preliminary querying OpenCL for the
(device-dependent) preferred work-group size multiple (i.e. the
warp/wavefront size in NVIDIA/AMD GPUs), $w_s$, and therefore by
considering the smallest multiple of $w_s$ which is greater than or
equal to the model dimension. For instance, if $w_s=32$ and the
first dimension of the domain is 2000, the number of work-items in
that dimension will be 2016, i.e. the first multiple of 32 which is
greater than or equal to 2000, thus resulting in 16 redundant
work-items. However, since redundant work-items do not map any cell
of the computational domain, they immediately terminate their
execution. Moreover, according to OpenCL, work-items are grouped in
workgroups. The choice of the number of workgroups to be considered,
and therefore the workgroup size, depends on the device architecture
and is automatically determined by default, transparently to the
user. These choices should allow to not waste resources and also
permits the user to ignore low-level hardware details. In any case,
the \verb'calclSetWorkGroupDimensions{2D|3D}()' function can be used
to explicitly set the workgroup size.

Double layer substates are also considered in OpenCAL-CL. That is,
as for the serial and OpenMP-based versions of OpenCAL, the current
layer is used for reading the states of the neighboring cells, while
the next for updating the new value for the central one. A grid of
OpenCL work-items can therefore be defined, each one executing the
transition function on a different cell of the computational domain,
independently to each other. In fact, no race conditions can occur,
since access to the current layer is read-only, and each work-item
updates a different memory location of the next layer. Being the
data stored in global memory, work-items can be easily synchronized
after the execution of each elementary process, since a global
barrier is implicitly defined at the end of each kernel
execution. The kernel-based transition function is therefore applied
by considering the one-thread/one-cell parallel execution policy
and, at the end of each computational step, substates are updated
device side, avoiding the need to perform time consuming data
transfer between host and device.

The quantization feature is also supported in OpenCAL-CL, where
coordinates of the active cells are stored in the array $A$ that,
differently from the serial and OpenMP-based implementations, is
static and has the same dimension of the computational domain. A
variable, $size$, initially set to zero, is also considered to
identify the actual number of active cells. A static array of flags,
$F$, is also considered, which has the same dimension of the
computational domain. It is initially set to \verb'CAL_FALSE' in
each position, to mark all cells as inactive. The add/remove
operations, performed by work-items at the init stage or during the
execution of the transition function, update the array of flags $F$
as in the previous discussed implementations. In case of concurrent
access to the same location of $F$, data integrity is guarantied
only under the condition work-items execute the same instruction
(e.g. all of them write the \verb'CAL_TRUE' value). In fact, if the
same instruction is executed by more than one work-item (even
belonging to different workgroups) to the same location in global
memory where $F$ is stored, the access is serialized and at least
one access is guarantied (even if which actual thread performs the
operation is undefined - cf. e.g. \cite{CUDA}). Moreover, the same
considerations discussed for the OpenCAL and OpenCAL-OMP
implementations of the active cells optimization, even hold for the
OpenCAL-CL one. As a consequence, to both guaranty data integrity
and to avoid possible logical errors, the add and remove stages have
to be performed separately, e.g. by two different elementary
processes. At the end of each add or remove stage, cells to be
considered active are marked by the \verb'CAL_TRUE' value in the
scattered array $F$. In order to update $A$, a parallel stream
compaction algorithm is considered. $F$ is partitioned in $N$
chunks, being $N$ the number of considered work-items, and the
following three stages applied:
\begin{enumerate}
	\item Each work-item counts the number of active cells in its chunk
	of the array $F$. As a result, an array $S$ is obtained, where
	$s_p$ ($p = 0, 1, \ldots, N-1$) is the number of the active cells
	counted by the $p^{th}$ work-item;
	\item A prefix-sum algorithm is executed to both evaluate the total
	number of active cells, $size$, and a further array, $O$, where
	$o_p$ ($p = 0, 1, \ldots, N-1$) represents the offset to be
	considered by the $p^{th}$ work-item from which it must start
	entering in $A$ the coordinates of the cells that are marked as
	active in its chunk of the array $F$.
	\item Each work-item processes its chunk of $F$ and enters in $A$
	the coordinates of the cells marked as active, starting from the
	offset $o_p$ ($p = 0, 1, \ldots, N-1$) computed in the previous
	stage.
\end{enumerate}
For illustrative purposes, the above three stages are graphically
represented in Figure \ref{fig:streamcompaction}. At step $t$, the
configuration of the cells actually involved in the computation are
represented in gray in a two-dimensional 4x4 matrix, corresponding
to the linearized scattered array $F$. The parallel stream
compaction algorithm processes the scattered array $F$ to obtain the
compacted array $A$, containing the coordinates of the three active
cells in its first three positions. A two-block grid with two
threads per block is considered in the example, which adopt the one
thread/one active cell execution policy. Note that, at step $t$ a
total of four work-items are considered to process a set of three
active cells. In this case, the thread that does not match any
active cell immediately terminates. Among the parallel stream
compaction stages, the prefix-sum algorithm at stage two, which
evaluates the array $O$ of offsets, is crucial for the overall
efficiency of the algorithm. 
\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./images/opencal/Figure11.pdf}
		\caption[An example of application of the prefix-sum algorithm used to build the array of \textit{active cells}.]{An example of application of the prefix-sum algorithm used to evaluate the array $O$ of offsets, needed to build the array $A$ of active cells. In the up-sweep phase, the array $O$ is initialized to the
		values of the array $S$, where $s_p$ is the number of cells to be
		added to $A$ by the $p^{th}$ wor-item. The array $O$ is therefore
		efficiently processed by a set of work-items and, at the end of the
		phase, the last cell of the array contains the total number of active cells to be considered for the next computational step. The down-sweep phase follows, in which dotted arrows are used to set the pointed cell of the array $O$ to $0$, while continuous arrows to evaluate the sum of the source cells and then to write the computed value to the pointed position, as in the previous phase. Offsets are eventually obtained at the end of the down-sweep phase. The first work item will therefore start adding $S_0$ cells to $A$ from the index $0$, the second adding $S_1$ cells starting from the offset $S_0$, the third adding $S_2$ cells from the offset $S_0+S_1$, up to the latter, which will add $S_7$ cells starting from the offset $S_0+S_1+\ldots+S_6$.}
		\label{fig:prefixsum}
	\end{center}
\end{figure}
As shown in Figure \ref{fig:prefixsum},
the algorithm takes as input the array $S$ of partial sums and uses
it to initialize the array $O$ of offsets. This latter is considered
as a balanced tree, where its elements are the nodes. In the first
phase, the tree is crossed from the leaves to the root (up-sweep
phase) by calculating, for each level, the partial sums of the nodes
of the previous level (by a parallel reduction pattern). Here the
total number of work-items is set to $N/2$, which means that a
thread will elaborate two elements of the array. The total number of
active cells, necessary to set the $size$ variable, is obtained at
the end of the up-sweep phase in the root node (i.e. in the last
element of the array). In the second phase, the tree is traversed
from the root to the leaves (down-sweep phase). At each iteration,
each node sets the value of the right child to the sum of its value
and the value of the left child. In addition, each node sets the
value of its left child to its value. At the end of this phase, the
array $O$ contains at each location the position from which each
work-item can write the coordinates of the active cells in its chunk
of $F$ to the array $A$. As known, the parallel prefix-sum algorithm
has a $O(log_2 N)$ computational efficiency, and is well suitable
for parallel execution, resulting in a fast solution for the second
stage of the parallel stream compaction algorithm. The overall
computational complexity of the OpenCAL-CL parallel stream
compaction, as for the OpenCAL and OpenCAL-OMP implementations, is
however $O(n)$, being $n$ the number of cells of the computational
domain. Due to its efficiency, the up-sweep phase of the parallel
prefix-sum algorithm is also applied to implement parallel global
reductions on substates (cf. Section \ref{sec:OpenCAL}).
\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./images/opencal/Figure10}
		\caption[\texttt{OpenCL} stream compaction algorithm adopted in \texttt{OpenCAL}.]{An example of application of the OpenCAL-CL parallel stream compaction algorithm. Active cells are represented in gray within a
			two-dimensional 4x4 matrix of flags, implemented as a linearized
			array, $F$. The parallel stream compaction algorithm processes $F$ and produces the compacted array $A$ as output, containing the coordinates of the active cells in its first part. A grid of work-items therefore processes data by adopting the one thread/one active cell policy. The process is therefore repeated at the next computational step.}
		\label{fig:streamcompaction}
	\end{center}
\end{figure}


\subsection{OpenCAL-CL device-side kernels}
Differently to OpenCL, where a kernel can have no parameters,
OpenCAL-CL ones must have at least the \verb'__CALCL_MODEL_2D'
meta-parameter (cf. line 8 of Listing
\ref{lst:OpenCAL-CL-kernel}). Actually, this is a macro-like C
object, defining a list of pre-fixed typed parameters, needed to
let the kernel access the model data device-side. Moreover, the
\verb'calcl[Active]ThreadCheck{2D|3D}()' function must be called
within any elementary process implemented as a kernel to prevent
the execution of a number of threads out of the computational
domain (cf. line 11) or of the set of active cells. The
\verb'calclGlobal{Row|Column|Slice}()' function (cf. lines 15-16)
have also to be used to get the global cell coordinates, which
here do not appear in the kernel parameter list. The
\verb'calclGet[X]{2D|3D}{b|i|r}()' and
\verb'calclSet{2D|3D}{b|i|r}()' functions are used for reading and
updating purposes. Differently to their host-side counterparts,
they take the \verb'MODEL_{2D|3D}' macro-like C meta-object, which
implicitly defines a list of required prefixed parameters
(cf. e.g. line 23). Moreover, substates are referred by means of
numerical handles (cf. the second parameter of the
\verb'calclSet2Dr()' function at line 23), which have to be
previously defined in the kernel (cf. line 6). The criterion to be
adopted is very simple: handles are zero-based IDs, i.e. zero is
used to refer the first substate registered to the host-side
model, one to refer the second substate, and so on. Different
zero-based handles must be defined for different typed substates.

\lstset{language=[OpenCL]C,
	caption=Example of OpenCAL-CL kernel., 
	label=lst:OpenCAL-CL-kernel, 
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
#include <OpenCAL-CL/calcl2D.h>

// Define substates handles
// omissis ... 
#define Z 4
#define H 5

__kernel void calcl_kernel_example(__CALCL_MODEL_2D)
{
// Prevent the execution of more threads than the CA dimension
calclThreadCheck2D();

// omissis ...

// Get the cell coordinates back
CALint i = calclGlobalRow();
CALint j = calclGlobalColumn();

// omissis ...

// Set a new value for the substate
// whose handle is defined by H.
// Please, note the usage of the
// MODEL_2D macro-like object
calclSet2Dr(MODEL_2D, H, i, j, h_next);

// omissis ...
}
\end{lstlisting}

\subsection{OpenCAL-CL host-side Programming}
An OpenCAL-CL host application is typically subdivided in the
following parts:
\begin{itemize}
	\item Definition of the host-side computational model;
	\item Selection of the OpenCL compliant device;
	\item Kernels reading and program generation;
	\item Definition of the device-side computational model (which
	also embeds simulation facilities);
	\item Kernels enqueuing;
	\item Simulation execution (on the previously selected compliant
	device).
\end{itemize}

The OpenCAL-CL host-side model definition does not differ from the
serial implementation of OpenCAL. Indeed, in Listing
\ref{lst:host-side-application}, a two-dimensional host-side model
object is declared by using the \verb'CALModel{2D|3D}' data type
(line 4), and then initialized by means of the
\verb'calCADef{2D|3D}()' function (line 11). Note that the
\verb'calcl{2D|3D}.h' header file is included at line 1. This, in
turn, includes the \verb'cal{2D|3D}.h' header, so that it is
possible to use OpenCAL data types and functions from an
OpenCAL-CL host application.

\lstset{language=[OpenCL]C,
	caption={An example of OpenCAL-CL host-side application.}, 
	label=lst:host-side-application, 
	basicstyle=\footnotesize\ttfamily,
keywordstyle=\color{ultramarine}\ttfamily,
stringstyle=\color{rosemadder}\ttfamily,
commentstyle=\color{outerspace}\ttfamily,
backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
#include <OpenCAL-CL/calcl2D.h>

// omissis ...

struct CALModel2D* hostCA;

// omissis ...

int main(int argc, char** argv)
{
// omissis ...

hostCA = calCADef2D(ROWS, COLS, CAL_VON_NEUMANN_NEIGHBORHOOD_2D, CAL_SPACE_TOROIDAL, CAL_OPT_ACTIVE_CELLS);

// omissis ...

}
\end{lstlisting}

OpenCAL-CL provides the \verb'CALCLManager' structure which,
together with other utility functions, considerably simplifies
platform, device, and context management with respect to the
native OpenCL API. Listing \ref{lst:CALOpenCL} shows how to select
a compliant device in OpenCAL-CL. Line 7 declares a pointer to the
\verb'CALCLManager' OpenCAL-CL data type, and initializes it
through the \verb'calclCreateManager()' function. This object,
\verb'calcl_manager', is then used as parameter for the
\verb'calclInitializePlatforms()' function (line 10), which fills
the object with the platforms available on the machine. Line 13
calls the \verb'calclInitializeDevices()' function, that
initializes the available devices, while line 20 selects one of
them for kernel execution. Specifically, an object of type
\verb'CALCLdevice' is declared and initialized by the function
\verb'calclGetDevice()'. This latter takes a pointer to a
\verb'CALCLManager' object as first parameter, while the second
and third parameters specify the platform and device to be
selected, respectively. Since both platforms and devices are
identified by a 0-based index, statement at line 20 selects the
first device belonging to the first platform (e.g., a GTX 980
belonging to the Nvidia CUDA platform). If system platforms and
devices are unknown, the
\verb'calclGetPlatformAndDeviceFromStdIn()' function can be used
alternatively to \verb'calclGetDevice()'. It prints all the
available platforms and devices to standard output and permits for
their interactive selection from standard input. Eventually, line
23 creates an OpenCL context, based on the device previously
selected. For this purpose, an object of \verb'CALCLcontext' type
is declared and defined by means of the
\verb'calclCreateContext()' function.


\lstset{language=[OpenCL]C,
	caption=Example of OpenCAL-CL access to platform and devices., 
	label=lst:CALOpenCL, 
	basicstyle=\footnotesize\ttfamily,
keywordstyle=\color{ultramarine}\ttfamily,
stringstyle=\color{rosemadder}\ttfamily,
commentstyle=\color{outerspace}\ttfamily,
backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
#include <OpenCAL-CL/calcl2D.h>

// omissis ...

int main (int argc, char** argv)
{
// Initilize a pointer to the CALCLManager structure
CALCLManager* calcl_manager = calclCreateManager();

// get all available platforms
calclInitializePlatforms(calcl_manager);

// Initialize the devices
calclInitializeDevices(calcl_manager);

// Uncomment if platforms and devices are unknown
//calclGetPlatformAndDeviceFromStdIn();

// get the first device on the first platform
// this call is unnecessary if
// calclGetPlatformsAndDeviceFromStandardInput() is used
CALCLdevice device = calclGetDevice(calcl_manager, 0, 0);

// create a context CALCLcontext
context = calclCreateContext(&device);

// omissis ...
}
\end{lstlisting}

Once the compliant device has been selected and functions to be
executed in parallel implemented as kernels, these latter can
automatically be read and compiled through the
\verb'calclLoadProgram{2D|3D}()' function. It takes both the
context and device, and also the paths to the directory containing
the user defined kernels and related headers (if any), and returns
an OpenCL program. All the files in the kernel source directory
are automatically loaded. Note that, since kernel headers are
optional, the last parameter can be \verb'NULL'.

The device-side counterpart of the host-side computational model
can be declared as a pointer to \verb'CALCLModel{2D|3D}' and,
beside managing all the host model components device-side, also
embeds simulation execution facilities. In this manner, the user
can continue to deal with only two main structures, as in the
serial and OpenMP-based implementations. The
\verb'calclCADef{2D|3D}()' function can be used to initialize the
device-side model object. It takes a pointer to an host-side
\verb'CALModel{2D|3D}' serial model, an OpenCL context, an OpenCL
program, and a compliant device as parameters.

Kernels can be extracted from an OpenCL program by means of the\hfil \\
\verb'calclGetKernelFromProgram()' function and then registered to
the device-side model by means of the
\verb'calclAddElementaryProcess{2D|3D}()' function, which adds the
kernel to the execution queue, in a transparent manner to the
user. The function takes a pointer to a host, a device model and
also a pointer to an OpenCL kernel. Global functions can also be
registered to the device model. For instance, the
\verb'calclAddInitFunc{2D|3D}()' function registers a global
initialization kernel, the \verb'calclAddSteeringFunc{2D|3D}()'
function registers a global kernel to be executed at the end of
each computational step, while the
\verb'calclAddStopConditionFunc{2D|3D}()' function registers a
global stop condition kernel callback.

The \verb'calclRun{2D|3D}()' function runs the simulation by
executing all the defined kernels on the selected compliant
device. The first two parameters are pointers to a device and host
models, respectively, while the last two are the initial and final
step for the simulation execution. If the last parameter is set to
\verb'CAL_RUN_LOOP', the simulation never ends. In this case, a
stop condition criterion must defined by registering a termination
kernel callback to halt the simulation.


\subsection{OpenCAL-CL implementation of the Game of Life}
According to OpenCAL-CL, the Game of Life implementation here
described is subdivided in a device- and an host-side part. The
device-side kernel implementing the Conway's Game of Life
transition function is shown in Listing
\ref{lst:calcl_life_kernel}. The \verb'calcl2D.h' is included at
line 1, and a numeric handle defined at line 3 to refer the $Q$
substate device-side. The transition rules are implemented as an
elementary process kernel at lines 5-23. In particular, line 7
checks for redundant work-items, while lines 9-10 get the
indices corresponding to the integer coordinates of the cell the
kernel is going to process. Similarly, line 12 retrieves the
neighborhood size by means of the
\verb'calclGetNeighborhoodSize()' function. Eventually, lines
16-22 implement the transition rules by using the
\verb'calclGet[X]2Di()' and \verb'calclSet2Di()' functions for
reading and updating purposes, respectively.

\lstset{language=[OpenCL]C,
	label=lst:calcl_life_kernel,
	caption=The OpenCAL-CL kernel implementing the Conway's Game of Life elementary process.,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\lstinputlisting{./code/calcl_life_kernel.c}

The host-side application, running on the CPU and controlling
the computation on the compliant device (e.g. a GPU), is shown
in Listing \ref{lst:calcl_life}.  The \verb'calcl2D.h' header
file is included, together with the OpenCAL \verb'cal2DIO.h'
header for I/O operations at lines 1-2. The kernel path is
defined at line 4, while the name of the kernel considered in
this example is defined at line 5. Lines 6-8 define the IDs of
the OpenCL platform and device to be considered. For the sake of
simplicity, in this example the first device belonging to the
first platform is considered. Lines 12-15 are needed to select
the compliant device and to create an OpenCL context. These
statements widely simplify the device management and can be
considered as a kind of template to be used in each OpenCAL-CL
application. Line 16 reads kernels (actually, just one in this
example) from file (contained in the directory specified at line
4), compile and groups them into an OpenCL program, to be used
later to extract kernels for execution. As in the serial
implementation of the Game of Life, the \verb'host_CA' host-side
object is defined at line 18 and the $Q$ substate declared at
line 19. This latter is therefore registered to the host-side CA
at line 21. Eventually, the substate is set to zero in each cell
and a glider is defined at lines 23-28. Line 30 defines the
\verb'device_CA' device-side object. The \verb'calclCADef2D()'
function initializes the device-side CA, by performing data
transfer from host to device, in a transparent way to the
user. Note that this function implicitly registers each
host-side defined substate to the device object. In order to
register an elementary process to the device-side CA, the
elementary process (which actually is an OpenCL kernel) must be
preliminarily extracted from the previously compiled
program. This operation is done at line 32 by means of the
\verb'calclGetKernelFromProgram()'. It returns an OpenCL kernel,
which is subsequently registered to the device CA by means of
the \verb'calclAddElementaryProcess2D()' function at line
33. Lines 35 and 39 are used to save the CA state before and
after simulation execution, respectively. The CA simulation, for
one step, is executed by means of the \verb'calclRun2D()'
function at line 37. In this example, the only defined
elementary process is executed in parallel on the compliant
device in a transparently way to the user. Eventually, lines
41-43 perform memory deallocation for the previously defined
objects. The return statement at line 45 terminates the program.

\lstset{language=[OpenCL]C,
	label={lst:calcl_life},
	caption=An OpenCAL-CL host-side implementation of the Conway's
	Game of Life.,
	basicstyle=\fontfamily{pcr}\selectfont\footnotesize,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\lstinputlisting{./code/calcl_life.c}

As for the case of the OpenCAL implementation of the Game of
Life, Figure \ref{fig:cal_life_glut} shows the initial and final
configuration of the system.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%SCIDDICA%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The $SciddicaT_{naive}$ Example of Application}
\label{sec:SciddicaT-naive}

In this section we show the OpenCAL, OpenCAL-OMP and OpenCAL-CL
implementations of a more complex example, namely the
$SciddicaT_{naive}$ fluid-flow XCA computational model. This is a
simplified version of the XCA model described in
\cite{avolio2000simulation} and is able to simulate the dynamics of
a generic non-inertial fluid-type flow over a real topographic
surface. The model is formally defined and key implementation
sections reported and commented. Eventually, the application to the
simulation of a real case study, namely the \textit{1992 Tessina (Italy)}
landslide, is shown.

\subsection{The $SciddicaT_{naive}$ Formal Definition}
\label{sec:sciddicaT_model}
The $SciddicaT_{naive}$ fluid-flow XCA computational model is
formally defined as:

$$SciddicaT_{naive} = < R, X, Q , P, \sigma >$$

where:

\begin{itemize}
	
	\item $R$ is the set of points, with integer coordinates, which
	defines the two-dimensional domain over which the phenomenon
	evolves. The generic cell in $R$ is individuated by means of a
	couple of integer coordinates $(i, j)$, where $0 \leq i < i_{max}$
	and $0 \leq j < j_{max}$. The first coordinate, $i$, represents
	the row, while the second, $j$, the column. The cell at
	coordinates $(0,0)$ is located at the top-left corner of the
	computational grid.
	
	\item $X = \{(0,0), (-1, 0), (0, -1), (0, 1), (1, 0)\}$ is the von
	Neumann neighborhood relation (cf. Figure
	\ref{fig:2Dneighborhood}a), a geometrical pattern which identifies
	the cells influencing the state transition of the central
	cell. The neighborhood of the generic cell of coordinate $(i, j)$
	is given by
	$$N(X, (i, j)) =$$
	$$= \{(i, j)+(0,0), (i, j)+(-1, 0), (i, j)+(0, -1), (i, j)+(0, 1),
	(i, j)+(1, 0)\} =$$
	$$= \{(i, j), (i-1, j), (i, j-1), (i, j+1), (i+1, j)\}$$
	
	Here, a subscript operator can be used to index cells belonging to
	the neighborhood. Let $|X|$ be the number of elements in X, and $n
	\in \mathbb{N}$, $0 \leq n < |X|$; the notation
	
	$$N(X, (i, j), n)$$
	
	represents the $n^{th}$ neighborhood of the cell $(i,j)$.
	
	\item $Q$ is the set of cell states. It is subdivided in the
	following substates:
	
	\begin{itemize}
		\item $Q_z$ is the set of values representing the topographic
		altitude (i.e. elevation a.s.l.);
		\item $Q_h$ is the set of values representing the thickness of the
		fluid;
		\item $Q_o^4$ are the sets of values representing the outflows
		from the central cell to the neighboring ones.
	\end{itemize}
	
	The Cartesian product of the substates defines the overall set of
	states $Q$:
	
	$$Q = Q_z \times Q_h \times Q_o^4$$ so that the cell state is
	specified by the following sextuplet:
	
	$$ q = (q_z, q_h, q_{o_0}, q_{o_1}, q_{o_2}, q_{o_3})$$ In
	particular, $q_{o_0}$ represents the outflows from the central
	cell towards the neighbor 1, $q_{o_1}$ the outflow towards the
	neighbor 2, and so on.
	
	\item $P$ is the set of parameters ruling the model
	dynamics where:	
	\begin{itemize}
		\item $p_\epsilon$ is the parameter which specifies the minimum thickness of fluid below which it cannot leave the cell due to the effect of adherence;
		\item $p_r$ is the relaxation rate parameter,
		which essentially is an outflow damping factor.
	\end{itemize}
	
	\item $\sigma : Q^5 \rightarrow Q$ is the deterministic cell
	transition function. It is composed by two elementary processes,
	listed below in the same order they are applied:
	\begin{itemize}
		\item $\sigma_1 : (Q_z \times Q_h)^5 \times p_\epsilon \times
		p_r\rightarrow Q_o^4$ determines the outflows from the central
		cell to the neighboring ones by applying the \emph{minimization
			algorithm of the differences} \cite{Gregorio1999}. In
		its simplest form, here considered, the algorithm is able to
		lead the neighborhood to the hydrostatic equilibrium in a single
		computational step. In the $\sigma_1$ elementary process, a
		preliminary control avoids outflows computation for those cells
		in which the amount of fluid is smaller or equal to
		$p_\epsilon$, acting as a simplification of the adherence
		effect. If $f(0,m) \; (m=0, \ldots, 3)$ represent the outgoing
		flows towards the 4 adjacent cells, as computed by the
		minimization algorithm, the resulting outflows are given by
		$q_o(0,m)=f(0,m) \cdot p_r$, being $p_r \in \; ]0,1]$ a
		relaxation factor considered to damp outflows in order to
		obtain a smoother convergence to the global equilibrium of
		the system. The $Q_o^4$ substates are accordingly updated.
		
		\item $\sigma_2: Q_h \times (Q_o^4)^4 \rightarrow Q_h$
		determines the value of debris thickness inside the cell by
		considering mass exchange in the cell neighborhood:
		$h^{t+1}(0) = h^t(0) + \sum_{m=0}^3 (q_o(0,m) -
		q_o(m,0))$. Here, $h^{t}(0)$ and $h^{t+1}(0)$ are the mass
		thickness inside the cell at $t$ and $t+1$ computational
		step, respectively, while $q_o(m,0)$ represents the inflow
		from the $n=(m+1)^{th}$ neighboring cell. The $Q_h$ substate
		is accordingly updated to account for the mass balance
		within the cell.
	\end{itemize}
\end{itemize}


\subsection{The $SciddicaT_{naive}$ OpenCAL and OpenCAL-OMP implementations}
According to OpenCAL/ and OpenCAL-OMP, the XCA programming model, substates,
parameters and the simulation object can be declared as:

\lstset{language=[OpenCL]C,
	caption={}, 
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
struct CALModel2D* sciddicaT;

struct sciddicaTSubstates {
struct CALSubstate2Dr *z;
struct CALSubstate2Dr *h;
struct CALSubstate2Dr *f[NUMBER_OF_OUTFLOWS];
} Q;

struct sciddicaTParameters {
CALParameterr epsilon;
CALParameterr r;
} P;

struct CALRun2D* sciddicaT_simulation;
\end{lstlisting}

\noindent where \verb'NUMBER_OF_OUTFLOWS' is equal to 4, since the
$\sigma_1$ elementary process computes 4 outflows towards the
adjacent cells belonging to the von Neumann neighborhood. Note that
the real-based substates and parameters are grouped in C structures
for convenience.

The model object is defined through the \verb'calCADef2D()'
function, by specifying the dimensions of the computational domain,
the neighborhood pattern, the boundary topology and the optimization
to be used:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
sciddicaT = calCADef2D(ROWS,
	COLS,
	CAL_VON_NEUMANN_NEIGHBORHOOD_2D,
	CAL_SPACE_TOROIDAL,
	CAL_NO_OPT
);
\end{lstlisting}

\noindent A toroidal domain is here defined even if, by considering
the application described below, a bounded one could be equivalently
adopted. Moreover, according to the model definition, the active
cell optimization is not employed.

Substates are therefore registered to the XCA model by means of the
\verb'calAddSubstate2Dr()' function:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
Q.z = calAddSubstate2Dr(sciddicaT);
Q.h = calAddSubstate2Dr(sciddicaT);
for (i = 0; i < NUMBER_OF_OUTFLOWS; i++)
	Q.f[i] = calAddSubstate2Dr(sciddicaT);
\end{lstlisting}

\noindent as well as elementary processes through the
\verb'calAddElementaryProcess2D()' function:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
calAddElementaryProcess2D(sciddicaT, flowsComputation);
calAddElementaryProcess2D(sciddicaT, widthUpdate);
\end{lstlisting}

\noindent where \verb'flowComputation()' and \verb'widthUpdate()'
are callback functions implementing the $\sigma_1$ and $\sigma_2$
elementary processes, respectively. A snippet of the $\sigma_1$
elementary process is shown below:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
void flowsComputation(struct CALModel2D* model, int i, int j)
{
CALreal f[NUMBER_OF_OUTFLOWS];

if (calGet2Dr(model, Q.h, i, j) <= P.epsilon)
	return;

computeMinimizingOutflows(f);

for (n=1; n<model->sizeof_X; n++)
	calSet2Dr(model, Q.f[n-1], i, j, f[n]*P.r);
}
\end{lstlisting}

\noindent The \verb'calGet2Dr()' function is here used to retrieve
the thickness of the fluid in the central cell, comparing its value
with the $p_\epsilon$ parameter for evaluating the adherence
condition. In case the thickness overcomes the adherence threshold,
the \verb'computeMinimizingOutflows()' function is called, which
applies the minimization algorithm of the differences (whose
implementation is here omitted) and returns the array of outgoing
flows, \verb'f'. Such flows are eventually damped by considering the
$p_r$ factor and the resulting values used to update the
corresponding substates by means of the \verb'calSet2Dr()'
function. The implementation of the $\sigma_2$ elementary processes
is also shown below:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
void widthUpdate(struct CALModel2D* model, int i, int j )
{
CALint n;
CALreal h_next = calGet2Dr(model, Q.h, i, j);

for(n=1; n<sciddicaT->sizeof_X; n++)
	h_next += calGetX2Dr(model, Q.f[NUMBER_OF_OUTFLOWS-n], i, j, n) - calGet2Dr(model, Q.f[n-1], i, j);

	calSet2Dr(model, Q.h, i, j, h_next);
}
\end{lstlisting}

\noindent Here, the \verb'calGetX2Dr()' function is used to get the
incoming flows from the neighbouring cells which, together with the
outgoing flows, are used to evaluate the mass balance and the
resulting value to update the $Q_h$ substate.

The above OpenCAL/OpenCAL-OMP code snippets completely define the
XCA model according to the $SciddicaT_{naive}$ formal definition. In
order to perform a simulation, a simulation object must be defined,
by means of the \verb'calRunDef2D()' function, permitting the model
evolve step by step:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
sciddicaT_simulation = calRunDef2D(sciddicaT,
	1,
	STEPS,
	CAL_UPDATE_IMPLICIT
);
\end{lstlisting}

\noindent The function takes the computational model to be carried out as
first parameter, the initial and final computational step and the
substates update policy. In this case, the implicit scheme is
considered, which demands the substates updating entirely to
\texttt{OpenCAL/OpenCAL-OMP}, transparently to the user. The simulation
object therefore registers two global callbacks for initialization
and steering purposes through the \verb'calRunAddInitFunc2D()' and
\verb'calRunAddSteeringFunc2D()' functions, respectively:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
calRunAddInitFunc2D(sciddicaT_simulation, simulationInit);
calRunAddSteeringFunc2D(sciddicaT_simulation, steering);
\end{lstlisting}

\noindent In particular, the \verb'simulationInit()' callback, listed below,
is executed once before the simulation loop (cf. Algorithm
\ref{algo:simuloop}) to define the initial condition of the
system. By referring the application described in the next section,
the callback simply subtracts the thickness of the mass, represented
by the $Q_h$ substate, from the surface over which it will
flow down, represented by the $Q_z$ substate:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
void simulationInit(struct CALModel2D* sciddicaT)
{
CALreal z, h;
CALint i, j;

for (i=0; i<sciddicaT->rows; i++)
	for (j=0; j<sciddicaT->columns; j++){
		h = calGet2Dr(sciddicaT, Q.h, i, j);
		z = calGet2Dr(sciddicaT, Q.z, i, j);

		calSet2Dr(sciddicaT, Q.z, i, j, z-h);
	}
}
\end{lstlisting}

\noindent Similarly, the steering callback, shown below, is executed at the
end of each computational step (cf. Algorithm \ref{algo:simuloop})
and is here simply used to reset the outflow substates, as needed,
by means of the \verb'calInitSubstate2Dr()' function:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
void steering(struct CALModel2D* sciddicaT)
{
for(n=0; n<NUMBER_OF_OUTFLOWS; n++)
	calInitSubstate2Dr(sciddicaT, Q.f[n], 0);
}
\end{lstlisting}

\noindent Eventually, the simulation is performed for \verb'STEPS'
computational steps by simply calling the \verb'calRun2D()'
function:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
calRun2D(sciddicaT_simulation);
\end{lstlisting}


\subsection{The $SciddicaT_{naive}$ OpenCAL-CL implementation}
\label{sec:sciddica_cl}
According to OpenCL, the OpenCAL-CL implementation of
$SciddicaT_{naive}$ differs from the one described in the previous
section as it is subdivided in two parts, the first running on the
CPU, the other on an OpenCL compliant device.

The computational model (here called \verb'host_CA'), substates and
parameters are defined host-side exactly as before, as well as the
initial conditions of the system. A device-side model is therefore
declared as an object of type \verb'CALCLModel2D':

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
struct CALCLModel2D* device_CA;
\end{lstlisting}

\noindent and then defined by means of the \verb'calclCADef2D()'
function:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
device_CA = calclCADef2D(host_CA, context, program, device);
\end{lstlisting}

\noindent which takes a pointer to the host-side model as first
parameter, needed for host-device data transfer purposes, while the
other ones are the OpenCL context, program and device, respectively.

The code to be executed device-side is defined as kernels. In
particular, the OpenCAL-CL implementation of $SciddicaT_{naive}$
define both the $\sigma_1$ and $\sigma_2$ elementary processes and
the steering global functions as kernels. The kernel defining the
$\sigma_1$ elementary process is defined as:
\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
__kernel void flowsComputation(__CALCL_MODEL_2D, __global CALParameterr* Pepsilon, __global CALParameterr* Pr )
{
calclThreadCheck2D();

int i = calclGlobalRow();
int j = calclGlobalColumn();

CALint sizeof_X = calclGetNeighborhoodSize();

CALreal f[5];

if (calclGet2Dr(MODEL_2D, H, i, j) <= *Pepsilon)
	return;

computeMinimizingOutflows(f);

for (n = 1; n < sizeof_X; n++)
	calclSet2Dr(MODEL_2D, n-1, i, j,f[n]*(*Pr));
}
\end{lstlisting}

\noindent Besides the mandatory \verb'__CALCL_MODEL_2D'
meta-parameter, the kernel takes two further parameters,
corresponding to the $SciddicaT_{naive}$ model parameters. Here, the
two additional parameters are located in the device global memory,
even if they could be stored in the (fast) local memory by simply
using the \verb'__local' qualifier instead of the \verb'__global'
one. The function \verb'calclThreadCheck2D()' is called first to
check if the work-item executing the kernel actually maps a cell of
the computational domain, where in such case cell coordinates are
retrieved by means of the \verb'calclGlobalRow()' and
\verb'calclGlobalColumn()' (kernel execution immediately terminates
in case of wrong mapping). The \verb'calclGet2Dr()' function is used
to retrieve the thickness of the fluid in the central cell, referred
by the \verb'H' numerical handle, which is therefore compared with
the $p_\epsilon$ parameter for evaluating the adherence
condition. In case the thickness overcomes the adherence threshold,
the \verb'computeMinimizingOutflows()' function is called, which
applies the minimization algorithm of the differences and returns
the array of outgoing flows, \verb'f'. Such flows are eventually
damped by considering the $p_r$ factor and the resulting values used
to update the values of the corresponding substates by means of the
\verb'calclSet2Dr()' function. As regards the optional kernels
parameters, they can be defined host-side by means of the
\verb'calclSetKernelArg2D()' function:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
calclSetKernelArg2D(&flow_computation_kernel,
	0,
	sizeof(CALParameterr),
	&P.epsilon
);

calclSetKernelArg2D(&flow_computation_kernel,
	1,
	sizeof(CALParameterr),
	&P.r
);
\end{lstlisting}

\noindent It takes the kernel as first argument, a 0-based handle
identifying the position of the parameter within the kernel
parameter list (the \verb'__CALCL_MODEL_2D' pseudo-parameter
excluded), and both the size and the host-side parameter.

Similarly to $\sigma_1$, the $\sigma_2$ elementary process is
defined by an OpenCAL-CL kernel as:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
__kernel void widthUpdate(__CALCL_MODEL_2D) {
calclThreadCheck2D();

int i = calclGlobalRow();
int j = calclGlobalColumn();

CALreal h_next;
CALint n;

h_next = calclGet2Dr(MODEL_2D, H, i, j);

for (n = 1; n < calclGetNeighborhoodSize(); n++)
	h_next += calclGetX2Dr(MODEL_2D, NUMBER_OF_OUTFLOWS-n, i, j, n) - calclGet2Dr(MODEL_2D, n-1, i, j);

	calclSet2Dr(MODEL_2D, H, i, j, h_next);
}
\end{lstlisting}

\noindent Here, the \verb'calclGetX2Dr()' function is used to get
the incoming flows from the neighbouring cells that, together with
the outgoing flows, are used to evaluate the mass balance and
therefore to update the $Q_h$ substate.

Once defined as kenels, elementary processes are added to the
device-side model by means of the
\verb'calclAddElementaryProcess2D()' function:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
calclAddElementaryProcess2D(device_CA, &flow_computation_kernel);
calclAddElementaryProcess2D(device_CA, &width_update_kernel);
\end{lstlisting}

\noindent Eventually, a steering kernel, whose implementation is
here omitted, can be added by means of the
\verb'calclAddSteeringFunc2D()' function:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
calclAddSteeringFunc2D(device_CA, &steering_kernel);
\end{lstlisting}

\noindent and the simulation performed for \verb'STEPS'
computational steps by means of the \verb'calclRun2D()' function:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
calclRun2D(device_CA, 1, STEPS);
\end{lstlisting}

\subsection{The $SciddicaT_{naive}$ Simulation of the Tessina Landslide}
Here we show the application of $SciddicaT_{naive}$ to the
simulation of the Tessina landslide \cite{avolio2000simulation} that
occurred in Northern Italy in 1992. The real case developed in the
Tessina valley between altitudes of 1220 m and 625 m a.s.l., with a
total longitudinal extension of nearly 3 km and a maximum width of
about 500 m. The landslide skimmed over the town of Funes and
stretched downhill as far as the village of Lamosano.

The topographic surface over which the landslide developed was
discretized as a DEM (Digital Elevation Model) of 410 rows per 294
columns, with square cells of 10 m side, for a total of 102,540
cells. The landslide source, specifying the location and thickness
of the detachment area, was also described by means of a raster map
of the same dimensions.

The $SciddicaT_{naive}$ parameters were set to the values listed in
table \ref{tab:sciddicaT-params} and a total of 4000 computational
steps considered in experiments. Simulation outcomes obtained by
considering the serial and the two parallel implementations of
$SciddicaT_{naive}$ did not differ, confirming the numerical
correctness of the OpenMP- and OpenCL- based implementation of
OpenCAL. Figure \ref{fig:sciddicaT-simulation} show the initial and
final configuration of the Tessina landslide, as obtained by the
$SciddicaT_{naive}$ simulation. It is worth to note that, even if
$SciddicaT_{naive}$ is a simplified model and parameters were
preliminary evaluated, simulation outcome is in agreement with that
of \citeauthor{avolio2000simulation}\cite{avolio2000simulation}.
\begin{table}
	\centering
	\begin{tabular}{ccc}
		\hline $SciddicaT$ parameter & Value & Unit \\ \hline
		$p_\epsilon$ & 0.001 & \si{m}\\ $p_r$ & 0.5 & 1\\ \hline
	\end{tabular}
	\caption{$SciddicaT$ parameters considered for the simulation of the 1992
		Tessina (Italy) landslide.}
	\label{tab:sciddicaT-params}
\end{table}

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./images/opencal/Figure05_new}
		\caption[The $SciddicaT$ simulation of the 1992 Tessina (Italy) landslide.]{The $SciddicaT$ simulation of the 1992 Tessina (Italy) landslide: (a) landslide source; (b) final landslide path. Topographic altitudes are represented in gray and vary between 1220 and 625 m a.s.l. Debris
		thickness is represented with colors ranging from red (lower values)
		to yellow (higher values).}
		\label{fig:sciddicaT-simulation}
	\end{center}
\end{figure}

\section{The $SciddicaT_{ac}$ Example of Application}
\label{sec:SciddicaT-ac}

In this section we show the OpenCAL, OpenCAL-OMP and OpenCAL-CL
implementations of $SciddicaT_{ac}$, a computationally improved
version of the $SciddicaT_{naive}$ fluid-flow XCA model which
exploits the OpenCAL active cell optimization feature. The model is
formally defined and key implementation sections reported and
commented. The application to the simulation of the 1992 Tessina
(Italy) landslide is here omitted since results are equivalent to
those obtained by $SciddicaT_{naive}$.

\subsection{The $SciddicaT_{ac}$ Formal Definition}
In the case of a fluid-flow model, only cells involved in mass
variation can be interested in a state change to the next
computational step. On the basis of this simple observation, we
can initialize the set of active cells to those cells containing
mass. Moreover, if during the computation an outflow is evaluated
from an active cell towards a neighboring non-active cell, this
latter can be added to the set of active cells and then considered
for subsequent state change. Similarly, if a given active cell
looses a sufficient amount of debris, it can be eliminated from
the set of active cells. In the case of $SciddicaT_{ac}$, this
happens when its thickness becomes lower than or equal to the
$p_\epsilon$ threshold.

In order to account for these processes, we have to slightly
revise the formal definition of the XCA fluid-flow model, by
adding the set of active cells, $A$. The optimized
$SciddicaT_{ac}$ model is now defined as:

$$SciddicaT_{ac} = < R, A, X, Q , P, \sigma >$$

\noindent where $A \subseteq R$ is the set of active cells, while
the other components are defined as in the formal definition of
$SciddicaT_{naive}$. The transition function is now defined as:

$$\sigma : A \times Q^5 \rightarrow Q \times A$$ denoting that it
is applied only to the cells in $A$ and that it can add/remove
active cells. More in detail, the $\sigma_1$ elementary process
has to be modified, as it can activate new cells. Moreover, a new
elementary process, $\sigma_3$, has to be added in order to remove
cells that cannot produce outflows during the next computational
step due to the fact that their debris thickness is
negligible. The new sequence of elementary processes is listed
below, in the same order they are applied.

\begin{itemize}
	\item $\sigma_1 : A \times (Q_z \times Q_h)^5 \times p_\epsilon
	\times p_r \rightarrow Q_o^4 \times A$ determines the outflows
	from the central cell to the neighboring ones, as in the case of
	$SciddicaT_{naive}$. In addition, each time an outflow is
	computed, the neighbor receiving the flow is added to the set of
	active cells.
	
	\item $\sigma_2: A \times Q_h \times (Q_o^4)^4 \rightarrow Q_h$
	determines the value of debris thickness inside the cell by
	considering mass exchange in the cell neighborhood. This
	elementary process does not differs with respect to that of the
	$SciddicaT_{naive}$ model.
	
	\item $\sigma_3: A \times Q_h \times p_\epsilon \rightarrow A$
	removes a cell from $A$ if its debris thickness is lower than or
	equal to the $p_\epsilon$ threshold.
\end{itemize}

\subsection{The $SciddicaT_{ac}$ OpenCAL and OpenCAL-OMP implementations}
Here we highlight the OpenCAL and OpenCAL-OMP differences between the $SciddicaT_{ac}$ and $SciddicaT_{naive}$ models. In particular, to properly exploit the active cells optimization, we have to change the definition of the CA object by using the \verb'CAL_OPT_ACTIVE_CELLS' parameter in the model
definition:
\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
sciddicaT = calCADef2D (ROWS,
		COLS,
		CAL_VON_NEUMANN_NEIGHBORHOOD_2D,
		CAL_SPACE_TOROIDAL,
		CAL_OPT_ACTIVE_CELLS
);
\end{lstlisting}

\noindent and by adding the $\sigma_3$ elementary process to the
model, in addition to the $\sigma_1$ and $\sigma_2$ ones:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
calAddElementaryProcess2D(sciddicaT, flowsComputation);
calAddElementaryProcess2D(sciddicaT, widthUpdate);
calAddElementaryProcess2D(sciddicaT, removeInactiveCells);
\end{lstlisting}

Preliminarly, at the \texttt{init} stage, cells belonging to the landslide
source are set as active by means of the
\verb'calAddActiveCell2D()' function:
\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
void simulationInit(struct CALModel2D* model){        
CALreal z, h;
CALint i, j;

for (i=0; i<model->rows; i++)
	for (j=0; j<model->columns; j++){
		h = calGet2Dr(model, Q.h, i, j);
		z = calGet2Dr(model, Q.z, i, j);

		calSetCurrent2Dr(model, Q.z, i, j, z-h);
		calAddActiveCell2D(model, i, j);
	}
}
\end{lstlisting}

\noindent and, when a flow is computed from the central cell
towards a neighbor by $\sigma_1$, the neighbor is added to $A$ by
means of the \verb'calAddActiveCellX2D()' function:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
void flowsComputation(struct CALModel2D* model, int i, int j )
{
// omissis...

for (n=1; n<model->sizeof_X; n++){
	calSet2Dr(model, Q.f[n-1], i, j, f[n]*P.r);
	calAddActiveCellX2D(sciddicaT, i, j, n);
	}
}
\end{lstlisting}

\noindent The $\sigma_2$ elementary process, here omitted, does
not differ from the one of $SciddicaT_{naive}$, while $\sigma_3$
is new, and is responsible to remove a cell if its debris
thickness is lower than or equal to the $p_\epsilon$ threshold:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
void removeInactiveCells(struct CALModel2D* model, int i, int j){
if (calGet2Dr(model, Q.h, i, j) <= P.epsilon)
	calRemoveActiveCell2D(model, i, j);
}
\end{lstlisting}

\noindent No other changes with respect to the $SciddicaT_{naive}$
implementation are needed.

Eventually, note that the active cells adding and remove stages
were implemented by two different elementary processes, $\sigma_1$
and $\sigma_3$ respectively, according to the considerations
discussed in Sections \ref{sec:OpenCAL} and \ref{sec:OpenCAL-OMP}.

\subsection{The $SciddicaT_{ac}$ OpenCAL-CL implementation}
To properly exploit the active cells optimization in the
OpenCAL-CL implementation of $SciddicaT_{ac}$, the host-side CA
object, here called \verb'host_CA', was defined as in the
OpenCAL/OpenCAL-OMP implementation, by using the
\verb'CAL_OPT_ACTIVE_CELLS' parameter in the definition. Moreover,
the $\sigma_3$ elementary process was added to the device-side
model, in addition to the $\sigma_1$ and $\sigma_2$ ones:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
calclAddElementaryProcess2D(device_CA, &flow_computation_kernel);
calclAddElementaryProcess2D(device_CA, &width_update_kernel);
calclAddElementaryProcess2D(device_CA, &rm_active_cells_kernel);
\end{lstlisting}

\noindent As for the OpenCAL and OpenCAL-OMP implementations,
cells belonging to the landslide source are set as active by means
of the \verb'calAddActiveCell2D()' at the init stage and when a
flow is computed from the central cell towards a neighbor by
$\sigma_1$, the neighbor is added to $A$ by means of the
\verb'calclAddActiveCellX2D()' function:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
__kernel void flowsComputation(__CALCL_MODEL_2D, __global CALParameterr* Pepsilon, __global CALParameterr* Pr )
{
// omissis ...

calclActiveThreadCheck2D();

// omissis ...

for (n = 1; n < calclGetNeighborhoodSize(); n++){
	calclSet2Dr(MODEL_2D, n-1, i, j,f[n]*(*Pr));
	calclAddActiveCellX2D(MODEL_2D, i, j, n);
	}
}
\end{lstlisting}

\noindent Here, note that the \verb'calclActiveThreadCheck2D()' is
called instead of the the \verb'calclThreadCheck2D()' function to
restrict the application of the elementary process to the cells
actually belonging to $A$. The $\sigma_2$ elementary process, here
omitted, does not differ from the one of $SciddicaT_{naive}$
OpenCAL-CL implementation, while the $\sigma_3$ one is new, which
is responsible to remove a cell if its debris thickness is lower
than or equal to the $p_\epsilon$ threshold:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
__kernel void removeInactiveCells(__CALCL_MODEL_2D, __global CALParameterr * Pepsilon )
{
// omissis ...
if (calclGet2Dr(MODEL_2D, H, i, j) <= *Pepsilon)
	calclRemoveActiveCell2D(MODEL_2D,i,j);
}
\end{lstlisting}

Eventually, as for the case of the OpenCAL/OpenCAL-OMP
implementation, the active cells adding and remove stages were
implemented by two different elementary processes, $\sigma_1$ and
$\sigma_3$ respectively, according to the considerations discussed
in Section \ref{sec:OpenCAL-CL}. Moreover, it is worth to note
that the number of active cells involved during the simulation of
the Tessina landslide vary between 637, corresponding to the
number of cells defining the landslide source, and 5,509. The
resulting mean value of active cell processed per step is 3,277,
corresponding to about the 3.2\% of the whole computational
domain. Figure \ref{gr:active_cells_count} shows how the number of
active cells varies when the $SciddicaT_{ac}$ computational step
is increased.

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./images/opencal/Figure13.pdf}
		\caption{Number of active cells over time for the considered $SciddicaT_{ac}$ simulation model of the Tessina landslide shown in Figure \ref{fig:sciddicaT-simulation}.}
		\label{gr:active_cells_count}
	\end{center}
\end{figure}

\section{The $SciddicaT_{ac+esl}$ Example of Application}
\label{sec:SciddicaT-ac+esl}

In this section we show the OpenCAL and OpenCAL-OMP implementations
of the further computationally improved $SciddicaT_{ac+asl}$
fluid-flow XCA model, which exploit both the active cell
optimization and the explicit simulation loop feature. The formal definition of the model does not differ from $SciddicaT_{ac}$, as well as
the application to the simulation of the 1992 Tessina (Italy)
landslide, and are not, therefore, reported in this section again.
The key implementation sections are reported and commented in the following section.

\subsection{The $SciddicaT_{ac+esl}$ OpenCAL and OpenCAL-OMP implementations}
Here we highlight the OpenCAL/OpenCAL-OMP few implementation
differences between the $SciddicaT_{ac+esl}$ and $SciddicaT_{ac}$
models. In particular, to properly exploit the explicit simulation
loop feature, which is able to override the predefined
OpenCAL/OpenCAL-OMP global transition function, by also allowing
for the selective update of model substates, we have to change the
definition of the CA simulation object by using the
\verb'CAL_UPDATE_EXPLICIT' parameter in its definition:
\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
sciddicaT_simulation = calRunDef2D(sciddicaT,
	1,
	STEPS,
	CAL_UPDATE_EXPLICIT
);
\end{lstlisting}

\noindent and also register a callback function to the simulation object to
implement the overridden global transition function:

\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
calRunAddGlobalTransitionFunc2D(sciddicaT_simulation,
overridedGlobalTransitionFunction
);
\end{lstlisting}

\noindent The overridden transition function is defined as:
\lstset{language=[OpenCL]C,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{ultramarine}\ttfamily,
	stringstyle=\color{rosemadder}\ttfamily,
	commentstyle=\color{outerspace}\ttfamily,
	backgroundcolor=\color{light-gray}, 
	numbers=left,numbersep=3pt,
	numberstyle=\tiny\ttfamily\color{gray}
}
\begin{lstlisting}
void overridedGlobalTransitionFunction( struct CALModel2D* model){
CALint i;

calApplyElementaryProcess2D(model, flowsComputation);
calUpdateActiveCells2D(model);
for (i=0; i<NUMBER_OF_OUTFLOWS; i++)
	calUpdateSubstate2Dr(model, Q.f[i]);

calApplyElementaryProcess2D(model, widthUpdate);
calUpdateSubstate2Dr(model, Q.h);

calApplyElementaryProcess2D(model, removeInactiveCells);
calUpdateActiveCells2D(model);
}
\end{lstlisting}

\noindent The \verb'calApplyElementaryProcess2D()' is used to
explicitly apply the elementary processes to the whole
computational domain or, as in this case, to the set of active
cells. Similarly, the active cells and substates updating must be
explicitly performed. These operations can be performed by
considering the \verb'calUpdate2D()' function, which updates both
the active cell structures and all the registered substates or, as
reported in this example, by means of the
\verb'calUpdateActiveCells2D()' and \verb'calUpdateSubstate2Dr()'
functions, which allows to restrict the update phase to only data
processed by the elementary processes. Indeed, the first
elementary process only processes the active cells structure and
the outflows substates, so that both the $Q_z$ and $Q_h$ substates
do not need to be updated. Similarly, since the second elementary
process only changes the debris thickness by evaluating the
incoming and outcoming flows mass balance, only the $Q_h$ substate
is updated. Similarly, since the last elementary process simply
removes cells that have become inactive from $A$, only the active
cells structure is updated.

Note that, even if their implementations are here omitted,
explicit updates have also to be performed after the execution of
each global functions.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{Validation, and Performance Results}
\label{sec:computational-results}

In order to evaluate OpenCAL from a computational point of view, the
different versions of $SciddicaT$ presented in the previous Section
were considered and the Tessina landslide taken into account as
simulation reference case study for a first set of tests
(\emph{standard tests}). In particular, a total of ten benchmark
simulations were executed for each of the nine $SciddicaT$
implemented versions, and the speed-up evaluated with respect to the
serial implementation of $SciddicaT_{naive}$, by considering the
minimum recorded execution times. Furthermore, in order to better
assess the impact of local memory usage in OpenCAL-CL, a further
implementation based on $SciddicaT_{naive}$ was considered, namely
$SciddicaT_{local}$. In this version, a 8 $\times$ 8 work-group size was
considered and data, i.e., the substates values of the cells
belonging to the neighborhood, transparently transferred
from the global to the fast local device memory by using the
\verb'calclGlobaltoLocal[X]()' API function (cf. Section
\ref{sec:OpenCAL}). In addition, due to the low transition function
computational intensity of $SciddicaT$ (i.e., the model is a more memory-bound rather than compute-bound application) and the data-set dimension,
which are not adequate to take significant advantage of the adopted
GPUs, two additional stress tests were carried out: the transition
functions were fictitiously made computationally heavier by
reapplying them, at each step, for a total of 200 times
(\emph{transition function stress tests}), and the landslide source
replicated for a total of 100 times over a wider computational
domain by considering a DEM of a total of 13,401,890 cells
(\emph{computational domain stress tests}). These latter tests were
also considered to evaluate the preliminary OpenCAL-MPI versions of $SciddicaT$,
both in terms of correctness and performance.

In all cases, OpenCAL and OpenCAL-OMP benchmarks were executed on a
8-core/16 threads Intel Xeon 2.0GHz E5-2650 CPU based
workstation. One thread was considered for testing the different
OpenCAL versions of $SciddicaT$, while 2, 4, 8 and 16 threads were
employed for benchmark experiments concerning OpenCAL-OMP
implementations. Moreover, two devices were adopted for testing the
different versions of the OpenCAL-CL implementations of $SciddicaT$,
namely a GTX 980 (Maxwell architecture) and a Tesla K40 (Kepler
architecture) graphic processor. In particular, the former has 2048
CUDA cores, 4 GB global memory and 112 GB/s theoretical bandwidth
communication for double precision data between CPU and GPU, while
the latter device has 2880 cores, 12 GB global memory and 144 GB/s
double precision high-bandwidth. Eventually, a Gigabit Ethernet
interconnected dual node test system with a GTX 980 GPU per node,
which is the configuration used for development purposes, was
considered for preliminary evaluating the OpenCAL-MPI versions of
$SciddicaT$ .


\subsection{Standard Tests}
\label{sec:standard_tests}
\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./images/opencal/Figure07_new}
		\caption{Speed-up obtained by the different OpenCAL-OMP
			versions of the $SciddicaT$ fluid-flow model. Elapsed times
			in seconds are also shown in correspondence of each speed-up
			vertex. The considered case study is the Tessina Landslide
			(cf. Figure \ref{fig:sciddicaT-simulation}). The adopted CPU
			was an Intel Xeon 2.0GHz E5-2650 CPU.}
		\label{gr:sciddicaT-OMP-absolute-speed-up}
	\end{center}
\end{figure}

\begin{table}
	\centering
	\begin{tabular}{cccc}
		\hline Threads & $SciddicaT_{naive}$ & $SciddicaT_{ac}$ &
		$SciddicaT_{ac+esl}$ \\ \hline 1 & 78.221 & 7.828 & 5.076\\ 2 &
		46.192 & 4.990 & 3.862\\ 4 & 29.321 & 3.287 & 2.501\\ 8 & 20.576
		& 2.745 & 1.698\\ 16 & 16.746 & 2.705 & 1.536\\ \hline
	\end{tabular}
	\caption{Elapsed times (in seconds) registered for the simulation of the
		Tessina Landslide (cf. Figure \ref{fig:sciddicaT-simulation}) by
		different OpenCAL and OpenCAL-OMP versions of the $SciddicaT$
		fluid-flow model. The adopted CPU is an Intel Xeon 2.0GHz E5-2650.}
	\label{tab:sciddicaT-OMP-execution-times}
\end{table}

The speed-up and execution times of the Tessina landslide
simulation related to the OpenCAL and OpenCAL-OMP different
versions of $SciddicaT$ are shown in Figure
\ref{gr:sciddicaT-OMP-absolute-speed-up} and reported Table \ref{tab:sciddicaT-OMP-execution-times}. Here, it is worth to
note how the optimizations progressively introduced are effective
and, as expected, execution times decrease steadily in all
cases. In fact, even in the case of the serial OpenCAL-based
implementations, the execution time decreases significantly from
about 78 seconds, registered by $SciddicaT_{naive}$, to about 5
seconds, for the fully optimized $SciddicaT_{ac+esl}$ version. As
expected, $SciddicaT_{ac+esl}$ is the version exhibiting the best
performance, running about 51 times faster on 16 threads with
respect to the reference simulation.

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./images/opencal/Figure08_new}
		\caption{Speed-up obtained by the different OpenCAL-CL
			versions of the $SciddicaT$ fluid-flow model. Elapsed times
			in seconds are also shown on top of each speed-up bar. The
			considered case study is the Tessina Landslide (cf. Figure
			\ref{fig:sciddicaT-simulation}). The adopted OpenCL
			compliant devices were a Nvidia Tesla K40 and an Nvidia GTX
			980.}
		\label{gr:sciddicaT-CL-absolute-speed-up}
	\end{center}
\end{figure}

\begin{table}
	\centering
	\begin{tabular}{cccc}
		\hline Device & $SciddicaT_{naive}$ & $SciddicaT_{local}$ &
		$SciddicaT_{ac}$ \\ \hline Tesla K40 & 11.108 & 10.605 &
		3.960\\ GTX 980 & 5.063 & 5.453 & 2.981\\ \hline
	\end{tabular}
	\caption{Elapsed times (in seconds) registered for the simulation of the
		Tessina Landslide (cf. Figure \ref{fig:sciddicaT-simulation}) by
		different OpenCAL-CL versions of the $SciddicaT$ fluid-flow model. The
		adopted OpenCL compliant devices are a Nvidia Tesla K40 and a Nvidia
		GTX 980.}
	\label{tab:sciddicaT-CL-execution-times}
\end{table}

The benchmark results of the OpenCAL-CL versions of $SciddicaT$
are instead shown in Figure
\ref{gr:sciddicaT-CL-absolute-speed-up} and reported in Table \ref{tab:sciddicaT-CL-execution-times}. Here, as expected,
$SciddicaT_{ac}$ resulted the more performing on both
devices. Unexpectedly, however, all the experiments executed on
the GTX 980 have outclassed simulations that were performed on the
Tesla K40, notwithstanding the first one being a gaming oriented
GPU, while the latter a HPC dedicated device. Even GPU hardware
issues might be taken into account: for instance, the K40, though
having more cores with respect to the GTX 980, has a lower CUDA
core clock-rate (745MHz vs 1126MHz) and lower memory clock-rate
(6008 MHz vs 7012 MHz). Also cache issues could justify the
results, since the K40 has less of both L1 and L2 level cache
memories than the GTX 980, this latter benefiting from Nvidia's
hardware improvements carried out in the more recent Maxwell
architectures with respect to the Kepler ones. Moreover,
independently from the adopted device, the $SciddicaT$ version
exploiting the GPU local memory did not resulted faster than the
corresponding global memory version. This can be justified by the
low transition function computational intensity, whereby
work-items do not access data in local memory a sufficient number
of times to result in better trade-off and thus better
performances.

In addition, in the case of $SciddicaT_{ac}$, it is worth to note
that the CPU performs better than the considered GPUs: 2.70
seconds on 16 threads, against 2.98 and 3.96 seconds on the GTX
980 and the Tesla K40, respectively. This can be explained by
considering that the mesh generated by the quantization algorithm
is too small to exploit the GPU latency thread hiding mechanism at
best \cite{Kirk2010}. In fact, the mean number of cells processed
per step is 3,277 (cf. Section \ref{sec:SciddicaT-ac}),
which is of the same order of magnitude of the number of cores of
the adopted GPUs (cf. above in this Section). This also leads to a
waste of bandwidth. In fact, while the other versions were able
able to adequately exploit the available bandwidth (e.g. the
$SciddicaT_{naive}$ version reached about 88 GB/s on the Tesla K40
GPU), the one exploiting the quantization optimization was not
able to take advantage of it (achieving 10 GB/s only). Eventually,
a further study performed on the most time consuming kernels has
shown that the achieved bandwidth is significantly higher for the
CPU. Particularly indicative is the value measured for the more
time consuming kernel, i.e. the one implementing the stream
compaction algorithm. This latter, which takes alone about the
55\% of the overall execution time on both the adopted CPU and
GPUs versions, exploits the bandwidth the 35\% better on the CPU,
while the other kernels are bandwidth equivalent or perform better
on the GPUs, all having however in this latter case a negligible
percentage of the overall execution time (about the 3\%). In other
words, the standard test case here considered is simply too small
to make decent use of the considered GPUs and, consequently it not
surprising that the CPU performs better in this specific case.

\subsection{Transition Function Stress Tests}    
As anticipated, in order to evaluate performances when considering
computationally intensive state transitions, further tests were
carried out by fictitiously increasing the complexity of the
$SciddicaT$ transition function. This was done by reapplying the
transition function $\sigma$ for a total of 200 times during each
simulation step, excluding data transfer (e.g. from global to
local memory, in the case of $SciddicaT_{local}$).

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./images/opencal/Figure09_new}
		\caption{Speed-up obtained during the \emph{transition
				function stress test} by the different OpenCAL-OMP
			versions of the $SciddicaT$ fluid-flow model. Elapsed times
			in seconds are also shown in correspondence of each speed-up
			vertex. The considered case study is the Tessina Landslide
			(cf. Figure \ref{fig:sciddicaT-simulation}). The adopted CPU
			was an Intel Xeon 2.0GHz E5-2650 CPU.}
		\label{gr:sciddicaT-OMP-absolute-speed-up-stress}
	\end{center}
\end{figure}

\begin{table}
	\centering
	\begin{tabular}{cccc}
		\hline Threads & $SciddicaT_{naive}$ & $SciddicaT_{ac}$ &
		$SciddicaT_{ac+esl}$ \\ \hline 1 & 8,665.033 & 303.904 &
		308.735\\ 2 & 6,240.054 & 221.756 & 217.155\\ 4 & 3,366.411 &
		111.761 & 113.116\\ 8 & 1,945.686 & 58.656 & 57.947\\ 16 &
		1,385.546 & 31.279 & 30.077\\ \hline
	\end{tabular}
	\caption[Elapsed times (in \si{s}) registered during the \emph{transition function stress test} by different OpenCAL and
	OpenCAL-OMP versions of the $SciddicaT$ fluid-flow model.]{Elapsed times (in \si{s}) registered during the \emph{transition function stress test} for the simulation of the Tessina Landslide
		(cf. Figure \ref{fig:sciddicaT-simulation}) by different OpenCAL and
		OpenCAL-OMP versions of the $SciddicaT$ fluid-flow model. The adopted
		CPU is an Intel Xeon 2.0GHz E5-2650.}
	\label{tab:sciddicaT-OMP-execution-times-stress}
\end{table}


Results of the benchmarks executed on the CPU are shown in Figure
\ref{gr:sciddicaT-OMP-absolute-speed-up-stress}, both in terms of
execution time and speed-up. Raw execution times for this benchmark are also reported in Table \ref{tab:sciddicaT-OMP-execution-times-stress}.
A more pronounced
timings decrease is here observed for each $SciddicaT$ version as
the number of threads is increased, with a maximum speed-up of
about 289 for the $SciddicaT_{ac+esl}$ execution on 16
threads. Here, the implementations exploiting the active cells
optimization outperform the naive one of two orders of magnitude.

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./images/opencal/Figure10_new}
		\caption{Speed-up obtained during the \emph{transition
				function stress test} by the different OpenCAL-CL versions
			of the $SciddicaT$ fluid-flow model. Elapsed times in
			seconds are also shown on top of each speed-up bar. The
			considered case study is the Tessina Landslide (cf. Figure
			\ref{fig:sciddicaT-simulation}). The adopted OpenCL
			compliant devices were a Nvidia Tesla K40 and an Nvidia GTX
			980.}
		\label{gr:sciddicaT-CL-absolute-speed-up-stress}
	\end{center}
\end{figure}
\begin{table}
	\centering
	\begin{tabular}{cccc}
		\hline Device & $SciddicaT_{naive}$ & $SciddicaT_{local}$ &
		$SciddicaT_{ac}$ \\ \hline Tesla K40 & 11.108 & 10.605 &
		3.960\\ GTX 980 & 5.063 & 5.453 & 2.981\\ \hline
	\end{tabular}
	\caption[Elapsed times (in seconds \si{s}) registered during the \emph{transition
		function stress test} by different OpenCAL-CL
	versions of the $SciddicaT$ fluid-flow model.]{Elapsed times (in seconds \si{s}) registered during the \emph{transition
			function stress test} for the simulation of the Tessina Landslide
		(cf. Figure \ref{fig:sciddicaT-simulation}) by different OpenCAL-CL
		versions of the $SciddicaT$ fluid-flow model. The adopted OpenCL
		compliant devices are a NVIDIA Tesla K40 and a Nvidia GTX 980.}
	\label{tab:sciddicaT-CL-execution-times-stress}
\end{table}
Figure \ref{gr:sciddicaT-CL-absolute-speed-up-stress} shows
instead, the benchmark results of the different OpenCAL-CL versions
of $SciddicaT$ on the considered graphic hardware for the
transition function stress test. Raw execution times are also reported in Table \ref{tab:sciddicaT-CL-execution-times-stress}. Here, conversely from the
standard tests, the $SciddicaT$ version exploiting the GPU local
memory resulted significantly faster with respect to the
corresponding global memory version on both the considered
devices. In particular, in this case, the Tesla K40 reported the
best result, evidencing a better local memory system (i.e., better
tradeoff between local memory access/transfer) with respect to the
GTX 980 GPU. Nevertheless, the $SciddicaT_{ac}$ performances
resulted always better than any CPU/GPU version (the GTX 980
performing better), demonstrating even in this case the validity
of the active cells optimization. It is worth to note that this time
the best GPU performance registered by the $SciddicaT$ OpenCAL-CL
versions significantly overcame the one registered on the CPU. In
particular, the $SciddicaT_{ac}$ ran about 441 times faster than
the serial version of $SciddicaT_{naive}$, against the best 289
speed-up registered on the CPU, pointing out, as expected, the
full suitability of GPGPU solutions in the case of sufficiently
computational intense simulation models.

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./images/opencal/Figure11_new}
		\caption{$SciddicaT$ simulation stress test of 100 landslide
			sources distributed over a DEM of 3593 rows per 3730
			columns, with square cells of 10 m side. Landslides paths
			are represented in black.}
		\label{fig:stresstestR}
	\end{center}
\end{figure}

\subsection{Computational Domain Stress Tests}
\label{sec:opencal_stress_testR}
In order to evaluate performances when larger computational
domains are taken into account, further tests were carried out by
considering a DEM of 3,593 rows per 3,730 columns, with square
cells of 10 m side. Moreover, the landslide source was uniformly
replicated 100 times over the extended DEM and a simulation
executed for each combination of $SciddicaT$ versions and
available devices. Figure \ref{fig:stresstestR} shows the
simulation outcomes obtained by considering the wider DEM and the
100 landslide sources.

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./images/opencal/Figure12_new}
		\caption{Speed-up obtained during the \emph{computational
				domain stress test} by the different OpenCAL-OMP versions
			of the $SciddicaT$ fluid-flow model. Elapsed times in
			seconds are also shown in correspondence of each speed-up
			vertex. The considered case study is the simulation shown in
			Figure \ref{fig:stresstestR}. The adopted CPU was an Intel
			Xeon 2.0GHz E5-2650 CPU.}
		\label{gr:sciddicaT-OMP-absolute-speed-up-stressR}
	\end{center}
\end{figure}
\begin{table}
	\centering
	\begin{tabular}{cccc}
		\hline Threads & $SciddicaT_{naive}$ & $SciddicaT_{ac}$ &
		$SciddicaT_{ac+esl}$\\ \hline 1 & 5,015.624 & 1,322.817 &
		724.035\\ 2 & 4,132.714 & 833.233 & 656.593\\ 4 & 3,271.501 &
		610.365 & 349.662\\ 8 & 2,943.584 & 478.984 & 272.623\\ 16 &
		2,794.782 & 412584 & 237.513\\ \hline
	\end{tabular}
	\caption[Elapsed times obtained for the \emph{computational domain stress test} by different OpenCAL and OpenCAL-OMP versions
	of the $SciddicaT$ fluid-flow model.]{Elapsed times (in seconds) obtained for the \emph{computational domain stress test} based on the simulation shown in Figure \ref{fig:stresstestR}) by different OpenCAL and OpenCAL-OMP versions
		of the $SciddicaT$ fluid-flow model. The adopted CPU is an Intel Xeon
		2.0GHz E5-2650.}
	\label{tab:sciddicaT-OMP-execution-times-stressR}
\end{table}
Computational results of the OpenCAL-OMP versions of $SciddicaT$
are reported in Figure
\ref{gr:sciddicaT-OMP-absolute-speed-up-stressR} and Table \ref{tab:sciddicaT-OMP-execution-times-stressR}. Similarly to the
standard tests, a slight timings decrease is observed for all
cases as the number of threads is increased. Values increase
accordingly to the adopted optimizations, resulting
$Sciddica_{ac+esl}$ the fastest version with a value of about 21.

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{./images/opencal/Figure13_new}
		\caption{Speed-up obtained during the \emph{computational
				domain stress test} by the different OpenCAL-CL versions
			of the $SciddicaT$ fluid-flow model. Elapsed times in
			seconds are also shown on top of each speed-up bar. The
			considered case study is the simulation shown in Figure
			\ref{fig:stresstestR}. The adopted OpenCL compliant devices
			were a Nvidia Tesla K40 and an Nvidia GTX 980.}
		\label{gr:sciddicaT-CL-absolute-speed-up-stressR}
	\end{center}
\end{figure}
\begin{table}
	\centering
	\begin{tabular}{cccc}
		\hline Device & $SciddicaT_{naive}$ & $SciddicaT_{local}$ &
		$SciddicaT_{ac}$ \\ \hline Tesla K40 & 909.664 & 704.518 &
		95.939\\ GTX 980 & 518.479 & 392.300 & 41.019\\ \hline
	\end{tabular}
	\caption{Elapsed times (in seconds) obtained for the \emph{computational domain stress test} based on the simulation shown in Figure
		\ref{fig:stresstestR}) by different OpenCAL-CL versions of the
		$SciddicaT$ fluid-flow model. The adopted OpenCL compliant devices are
		a Nvidia Tesla K40 and a Nvidia GTX 980.}
	\label{tab:sciddicaT-CL-execution-times-stressR}
\end{table}
Benchmark results of the OpenCAL-CL different versions of
$SciddicaT$ on the computational domain stress tests are instead
reported in Figure
\ref{gr:sciddicaT-CL-absolute-speed-up-stressR} and Table \ref{tab:sciddicaT-CL-execution-times-stressR}. Here, as for the
standard tests, the $SciddicaT$ version exploiting the GPU local
memory did not result significantly faster than the corresponding
global memory version on both the considered devices, by
confirming that local memory has to be accessed an elevated number
of times to take an effective advantage compared to the global one
(and thus can result more useful for higher computationally
complex models). However, the $SciddicaT_{ac}$ versions
performances resulted always better than any CPU/GPU version (the
GTX 980 performing better), demonstrating even in this case the
validity of the active cells optimization. Moreover, even in this
case, the best GPU performance overcame the one registered on the
CPU. In particular, the $SciddicaT_{ac}$ ran about 122 times
faster on the GTX 980 than the serial version of
$SciddicaT_{naive}$, against the best 21 absolute speed-up
registered on the CPU pointing out, as expected, the usefulness of
GPGPU solutions also in the case of extended computational domains. The
result is justified by the same evaluations performed for the
standard test case. In particular, the higher dimension of the
computational domain stress test mesh permits the GPUs to always
perform better than the CPU in terms of achiewed bandwidth for all
the considered $SciddicaT$ versions (the $SciddicaT_{ac}$ version
included, which achieves about 77 GB/s of bandwidth on the Tesla
K40, against the 10 GB/s achieved on the smaller mesh), by
consequently allowing to hide the thread latency in all cases, and
thus to better exploit the GPU computational power. Eventually, it
is worth to note that, as in the standard tests, the GTX 980
outperformed the Tesla K40, confirming that a gaming-oriented
device is a preferable solution in case of low-intense
computational models.


\section{Discussion and Outlooks}

In this chapter the first release of OpenCAL is presented, a new
open source computing abstraction layer for Scientific Computing,
currently supporting Cellular Automata, Extended Cellular Automata
and the Finite Differences computational methods.

Besides the serial implementation, two different parallel versions
were developed, namely OpenCAL-OMP and OpenCAL-CL, based on OpenMP
and OpenCL, respectively. The first one allows to exploit multi-core
CPUs on shared memory computers, while the second a wide range of
heterogeneous devices like GPUs, FPGAs and other many-core
coprocessors.

Each version was designed to be the most reliable and fast possible
and, for this purpose, the C language was adopted and efficient data
types and algorithms considered. In particular, also to permit a
more straightforward OpenCL parallelization, linearized arrays were
adopted to represent both one-dimensional and higher order
structures like substates and neighbourhoods. Moreover, the
quantization optimization, which allows to define the set $A$ of
non-stationary cells to which restrict the application of the
transition function, was implemented in each version. Specifically,
a straightforward stream compaction was considered in OpenCAL, which
serially checks the state of the cells in the computational domain,
by placing the coordinates of non-stationary cells in
$A$. OpenCAL-OMP essentially implements the same strategy, even if a
pool of threads preliminarily build a set of sub-arrays of active
cells in parallel, which are eventually assembled together to form
the final array $A$. A different algorithm was implemented in
OpenCAL-CL, where the parallel stream compaction relies on a
parallel prefix sum algorithm used to preliminary evaluate the
offsets to be used by work-items to fill the array of active cells
$A$ in parallel. In addition to the quantization optimization,
OpenCAL and OpenCAL-OMP were designed to allow for the explicitation
of the global transition function, by also allowing selective
updating of substates.

The SciddicaT XCA landslide simulation model was considered to show
the straightforward implementation of a computational model and also
to assess numerical correctness and computational efficiency of each
OpenCAL implementation. Specifically, the OpenCAL and OpenCAL-OMP
implementations of three different versions of $SciddicaT$ were
shown, from a naive one, $SciddicaT_{naive}$, to a version
supporting the quantization optimization, $SciddicaT_{ac}$, up to a
fully optimized version, $SciddicaT_{ac+esl}$, supporting both the
quantization and the explicitation of the global transition
function. The first two versions of $SciddicaT$ were also
implemented in OpenCAL-CL. In addition, a naive version of
$SciddicaT$ exploiting the local memory, namely $SciddicaT_{local}$,
was implemented in OpenCL to evaluate the role of different GPUs
memory levels.

For each $SciddicaT$ version, the Tessina landslide was considered
and a total of ten benchmarks (simulations) executed to evaluate
correctness and timings on each considered hardware configuration,
namely a 16 threads Intel Xeon CPU based workstation and two Nvidia
GPUs. Numerical correctness was confirmed by all the simulation
outcomes, which perfectly matched to the one of the OpenCAL
implementation of $SciddicaT_{naive}$ that was selected for
reference. Regarding computational performance, the different
$SciddicaT$ versions demonstrated to be able to efficiently exploit
the computational power of the heterogeneous devices considered in
this work, by reducing the execution time of all the performed
benchmarks accordingly to the progressively adopted
optimizations. However, the best result obtained by
$SciddicaT_{ac+esl}$ using 16 threads on the CPU surprisingly
doubled the best one obtained by $SciddicaT_{ac}$ on the GPU in
terms of absolute speed-up (i.e. computed with respect to the timing
of the reference simulation), probably due to the very low
computational complexity of the transition function and the
dimension of the computational domain. Nevertheless, subsequent
stress tests performed by fictitiously complicating the transition
function execution, and a further set of tests where the
computational domain was considerably increased with respect to the
one originally considered, overturned the results, and GPUs
significantly resulted faster than the CPU, pointing out their
usefulness in case of the simulation of computationally heavy
models. Eventually, as regards GPU local memory, it showed to
provide an actual advantage only in the case of the first set of
stress tests, pointing out that data must be accessed an adequate
number of times to be effective. Here, in particular, the Tesla K40
resulted more efficient with respect to the GTX 980, even if based
on the previous Nvidia hardware architecture, probably due to a
better management of the local memory.

Though preliminary, obtained results confirm correctness and
efficiency of the different OpenCAL versions here presented, by
highlighting their goodness for numerical model development of
complex systems in the field of Scientific Computing and their
execution on parallel heterogeneous devices. Moreover, since the
implementations do not significantly differ from an OpenCAL
implementation to another, it is easily possible to obtain two
different CPU/GPU parallel versions of the same model with a minimum
effort and, therefore, to test them on the available hardware to
select the best platform for execution. In fact, as shown for the
case of $SciddicaT$, the best choice can deeply depend on both the
computational complexity of the transition function and on the
extent of the computational domain, and the best solution can not be
determined \emph{a priori}.

Nevertheless, a fine tuning of underlying data structures and
algorithms will be performed in order to make OpenCAL still more
performing and MPI will be adopted to allow OpenCAL to exploit the
computational power of distributed memory systems. As regard the
OpenCL implementation, the seamless management of GPUs local memory
will be introduced in the next releases, and Multi-GPU support added
to intelligently scale the overall system performances. Subsequent
releases will also progressively support further computational
paradigms, like the Lattice Boltzmann, the Smoothed Particle
Hydrodynamics (SPH), as well as other mesh-free numerical methods,
with the aim to become a general software abstraction layer for
computation.

The OpenCAL software libraries, together with a comprehensive
installation and user manual accompanied by numerous examples, are
currently freely available on GitHub, at \url{https://github.com/OpenCALTeam/opencal}.

%https://books.google.it/books?id=g3EzsZn4poUC&pg=PA267&lpg=PA267&dq=multi+gpu+introduction&source=bl&ots=-2vAlqo3QT&sig=gL5DLWQNor9l7IbgrCKJXbQdEpc&hl=it&sa=X&ved=0ahUKEwi48dmwzrHWAhXFWBQKHbmmCCYQ6AEIaDAJ#v=onepage&q=multi%20gpu%20introduction&f=false


