\chapter{Introduction}


Over the last two decades, a lot has changed regarding the way modern scientific applications are designed, written and executed, especially in the field of data-analytics, scientific computing and visualization. The main reasons behind these changes are that the size of the problems that scientists try to tackle is nowadays much bigger and the amount of available raw data that can be analyzed has widened the spectrum of computing applications.
Data analytics and big-data techniques are applied in pretty much every field of science and have been exploited effectively also by governments and corporate organizations.

Traditionally, performance improvements in computer architecture have come from cramming more functional units onto silicon, increasing clock speeds and transistors number. Coupled with increasing clock speeds, CPU performance has until recently doubled every two years.  But it is important to acknowledge that this trend cannot be sustained indefinitely or forever. Increased clock speed and transistor number require more power and consequently generate more heat, at the point that the heat emitted from a modern processor, measured in power density, rivals the heat emitted by a nuclear reactor core!
But the demand of speed did not stop in over the years and is not going to stop in the near future,  and thus, from these reasons comes the necessity of relying heavily on parallel architectures.
Multi-core CPUs (2, 4, 8, 12, up to $40$)  are ubiquitous at the point that even smart-phones are proper multi-core machines.
Dedicated computing machines are nowadays large, powerful agglomerates of hundreds or thousands of multi-core computing nodes interconnected via  network each coupled with multiple accelerators.
Those kinds of parallel machines are very complex and their efficient programming is hard, bug-prone and time-consuming. 

In the field of scientific computing, and of modeling and simulation especially, parallel machines are used to obtain approximate numerical solutions to differential equations which describe a physical system rigorously, as for example for the \textit{Maxwell's} equations at the foundation of classical electromagnetism or the \textit{Navier-Stokes} for fluid dynamics.
The classical approach, based on calculus, often fails to solve these kinds of equations analytically, making a numerical computer-based approach absolutely necessary.
An approximate numerical solution of a partial differential equation can be obtained by applying a number of methods, as the finite element or finite difference method which yields approximate values of the unknowns at a discrete number of points over the domain.
When large domains are considered, large parallel machines are required in order to process the resulting huge amount of mesh nodes. Parallel programming is notoriously complex, often requiring great programming efforts in order to obtain efficient solvers targeting large computing cluster. This is especially true since heterogeneous hardware and GPGPU has become mainstream.

The main thrust of this work is the creation of a programming abstraction and a runtime library for seamless implementation of numerical methods on regular grids targeting different computer architecture: from commodity single-core laptops to large clusters of heterogeneous accelerators. A framework, \texttt{OpenCAL} had been developed, which exposes a domain specific language for the definition of a large class of numerical models and their subsequent deployment on the targeted machines. Architecture programming details are abstracted from the programmer that with little or no intervention at all can obtain a \textit{serial, multi-core, single-GPU, multi-GPUs and cluster of GPUs} \texttt{OpenCAL} application. 
Results show that the framework is effective in reducing programmer effort in producing efficient parallel numerical solvers.

The rest of the thesis is organized as follows:
Chapters \ref{ch:FDM} and \ref{ch:parallel_computing} introduce the main targeted numerical models and parallel architectures, respectively.
Chapters \ref{ch:opencal}  describes \texttt{OpenCAL}, its implementation and different versions, usage and performance on a number of benchmarks, while Chapter \ref{ch:opencal_cluster} introduces the multi-GPU and distributed memory version of \texttt{OpenCAL} and evaluates its performance on three  kinds of applications, each with different computational and memory requirements.
Eventually, Chapters \ref{ch:bacteria} and  \ref{ch:flocking} introduce other HPC numerical modeling and simulation applications that have been investigated. 
In particular, Chapter \ref{ch:bacteria} introduces a specialized framework based on \texttt{OpenCAL} for tracking particle-like objects from a time-lapse video which has been applied to analyze the motility of the \textit{B. subtilis} bacterium, while Chapter \ref{ch:flocking} investigates multi-agent collective system acceleration on GPU.
Appendix \ref{ch:stream_compaction} refers to an ad-hoc stream-compaction algorithm specifically targeting NVIDIA newest hardware that was investigated during the work on \texttt{OpenCAL}.
